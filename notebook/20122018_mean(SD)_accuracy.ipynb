{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda/envs/gc/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "from gcforest.gcforest import GCForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "kf=10\n",
    "score = 'f1_weighted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data_four_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33612, 5)\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()+'/../data/20122018freshwater_four_feature.csv'\n",
    "data_four_features = pd.read_csv(path, na_values = np.nan)\n",
    "\n",
    "print(data_four_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CE(1-9)标签y为1-6\n",
    "- CE(1-10)标签y为0-5\n",
    "\n",
    "先进行计算CE(1-9)，在载入CE(1-10)的时候要将标签统一减去1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "水质分布情况:\n",
      "2    13272\n",
      "3     8797\n",
      "4     5472\n",
      "1     2438\n",
      "6     2146\n",
      "5     1487\n",
      "Name: 本周水质, dtype: int64\n",
      "\n",
      "各特征类型分布情况:\n",
      "float64    4\n",
      "int64      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = data_four_features.drop(['本周水质'], axis=1) # Series\n",
    "y = data_four_features['本周水质'] # Series\n",
    "print(\"水质分布情况:\")\n",
    "print(y.value_counts())\n",
    "print(\"\\n各特征类型分布情况:\")\n",
    "print(data_four_features.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert series to ndarray\n",
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ train_test_split ============\n",
      "67% train: 26889/33612, 33% test: 6723/33612\n"
     ]
    }
   ],
   "source": [
    "print(\"============ train_test_split ============\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                       stratify = y, random_state = random_seed)\n",
    "print(\"67%% train: %d/%d, 33%% test: %d/%d\" %(X_train.shape[0], X.shape[0], X_test.shape[0], X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize  train data\n",
    "\n",
    "fulfill the Na with median, then standardized the data, output type ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ train_valid_split ============\n"
     ]
    }
   ],
   "source": [
    "clean_pipeline = Pipeline([('imputer', preprocessing.Imputer(missing_values='NaN',strategy=\"median\")),\n",
    "                           ('std_scaler', preprocessing.StandardScaler()),])\n",
    "X_train = clean_pipeline.fit_transform(X_train)\n",
    "X_test = clean_pipeline.fit_transform(X_test)\n",
    "\n",
    "print(\"============ train_valid_split ============\")\n",
    "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, test_size=0.25, \n",
    "                                       stratify = y_train, random_state = random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X_train2: training set\n",
    "- X_valid: validation set\n",
    "- X_test: test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy's MEAN(SD) based on 5 flod cross validation\n",
    "1. 载入CE（1-9）的子模型，仅计算CE（1-9）\n",
    "2. 载入CE（1-10）的子模型覆盖之前的模型，计算10个子模型\n",
    "3. 计算CE（1-10）\n",
    "4. 共得到12个模型的MEAN(SD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 载入CE（1-9）的子模型，仅计算CE（1-9），标签为1-6\n",
    "\n",
    "argmax后标签为0-5，需要加1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE(1-9)  Accuracy: 99.77 $\\pm$ 0.05 %\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    \"LogisticRegression\",\n",
    "    \"LinearDiscriminantAnalysis\",\n",
    "    \"SVC\",\n",
    "    \"DecisionTreeClassifier\",\n",
    "    \"ExtraTreeClassifier\",\n",
    "    \"GaussianNB\",\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"RandomForestClassifier\",\n",
    "    \"ExtraTreesClassifier\"\n",
    "]\n",
    "i=0\n",
    "for model in models:\n",
    "    model_name = model\n",
    "    with open(\"../pkl/CE_97661/CE_\" + model_name + \".pkl\", \"rb\") as f:\n",
    "        models[i] = pickle.load(f)\n",
    "    i = i+1\n",
    "# models 不再是字符数组，而是模型数组\n",
    "\n",
    "population_best_weight = np.load(\"../npy/CE_best_weights.npy\")\n",
    "\n",
    "classifier_num = 9\n",
    "\n",
    "scores = []\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    K_train_x, K_test_x = X_train[train_index], X_train[test_index]\n",
    "    K_train_y, K_test_y = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    # 所有学习器都输出概率向量，最后投票\n",
    "    y_test_pred_proba_all = []\n",
    "    # 取训练好的模型，计算各模型”验证集“上输出概率向量\n",
    "    for model in models:\n",
    "        model_name = model.__class__.__name__\n",
    "        y_test_pred_proba = model.predict_proba(K_test_x)\n",
    "        y_test_pred_proba_all.append(y_test_pred_proba)\n",
    "\n",
    "    y_test_pred_ensemble_proba = np.zeros((len(K_test_y), 6)) # 初始化集成器概率向量\n",
    "    \n",
    "    # 为每一个基学习器乘上权重\n",
    "    for k in range(classifier_num):\n",
    "        y_test_pred_ensemble_proba += y_test_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_test_pred_ensemble = np.argmax(y_test_pred_ensemble_proba, axis=1) + 1\n",
    "    \n",
    "    scores.append(accuracy_score(K_test_y, y_test_pred_ensemble))\n",
    "print(\"CE(1-9)  Accuracy: %0.2f $\\pm$ %0.2f %%\" % (np.mean(scores)*100, np.std(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 载入CE（1-10）的子模型覆盖之前的模型，计算10个子模型，标签为 0-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression    Accuracy: 66.73 $\\pm$ 0.75 %\n",
      "LinearDiscriminantAnalysis    Accuracy: 61.81 $\\pm$ 0.09 %\n",
      "SVC    Accuracy: 89.44 $\\pm$ 0.42 %\n",
      "DecisionTreeClassifier    Accuracy: 99.69 $\\pm$ 0.05 %\n",
      "ExtraTreeClassifier    Accuracy: 97.39 $\\pm$ 0.16 %\n",
      "GaussianNB    Accuracy: 81.44 $\\pm$ 0.48 %\n",
      "KNeighborsClassifier    Accuracy: 92.69 $\\pm$ 0.45 %\n",
      "RandomForestClassifier    Accuracy: 99.78 $\\pm$ 0.05 %\n",
      "ExtraTreesClassifier    Accuracy: 99.33 $\\pm$ 0.14 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-02-11 15:44:22,194][cascade_classifier.transform] X_groups_test.shape=[(5380, 4)]\n",
      "[ 2019-02-11 15:44:22,195][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 15:44:22,196][cascade_classifier.transform] X_test.shape=(5380, 4)\n",
      "[ 2019-02-11 15:44:22,197][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(5380, 4)\n",
      "[ 2019-02-11 15:44:22,335][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:22,488][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:22,632][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:22,772][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:22,909][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:23,052][cascade_classifier.transform] X_groups_test.shape=[(5380, 4)]\n",
      "[ 2019-02-11 15:44:23,053][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 15:44:23,054][cascade_classifier.transform] X_test.shape=(5380, 4)\n",
      "[ 2019-02-11 15:44:23,055][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(5380, 4)\n",
      "[ 2019-02-11 15:44:23,209][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:23,367][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:23,538][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:23,686][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:23,826][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:23,974][cascade_classifier.transform] X_groups_test.shape=[(5377, 4)]\n",
      "[ 2019-02-11 15:44:23,977][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 15:44:23,979][cascade_classifier.transform] X_test.shape=(5377, 4)\n",
      "[ 2019-02-11 15:44:23,982][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(5377, 4)\n",
      "[ 2019-02-11 15:44:24,118][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(5377, 52)\n",
      "[ 2019-02-11 15:44:24,268][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(5377, 52)\n",
      "[ 2019-02-11 15:44:24,415][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(5377, 52)\n",
      "[ 2019-02-11 15:44:24,568][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(5377, 52)\n",
      "[ 2019-02-11 15:44:24,729][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(5377, 52)\n",
      "[ 2019-02-11 15:44:24,873][cascade_classifier.transform] X_groups_test.shape=[(5376, 4)]\n",
      "[ 2019-02-11 15:44:24,874][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 15:44:24,874][cascade_classifier.transform] X_test.shape=(5376, 4)\n",
      "[ 2019-02-11 15:44:24,876][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(5376, 4)\n",
      "[ 2019-02-11 15:44:25,037][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:25,185][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:25,337][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:25,478][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:25,637][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:25,816][cascade_classifier.transform] X_groups_test.shape=[(5376, 4)]\n",
      "[ 2019-02-11 15:44:25,817][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 15:44:25,818][cascade_classifier.transform] X_test.shape=(5376, 4)\n",
      "[ 2019-02-11 15:44:25,819][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(5376, 4)\n",
      "[ 2019-02-11 15:44:25,953][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:26,098][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:26,239][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:26,388][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:26,530][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(5376, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCForest    Accuracy: 99.78 $\\pm$ 0.08 %\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train-1\n",
    "models = [\n",
    "    \"LogisticRegression\",\n",
    "    \"LinearDiscriminantAnalysis\",\n",
    "    \"SVC\",\n",
    "    \"DecisionTreeClassifier\",\n",
    "    \"ExtraTreeClassifier\",\n",
    "    \"GaussianNB\",\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"RandomForestClassifier\",\n",
    "    \"ExtraTreesClassifier\",\n",
    "    \"GCForest\"\n",
    "]\n",
    "\n",
    "\n",
    "i=0\n",
    "for model in models:\n",
    "    model_name = model\n",
    "    model_path = \"../pkl/CE_97661_10/CE_\" + model_name + \".pkl\"\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        models[i] = pickle.load(f)\n",
    "    \n",
    "    # Kfold\n",
    "    scores = []\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        K_train_x, K_test_x = X_train[train_index], X_train[test_index]\n",
    "        K_train_y, K_test_y = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "        y_pred = models[i].predict(K_test_x)\n",
    "        scores.append(accuracy_score(K_test_y, y_pred))\n",
    "    print(\"%s    Accuracy: %0.2f $\\pm$ %0.2f %%\" % (model_name, np.mean(scores)*100, np.std(scores)*100))\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算CE（1-10）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-02-11 15:44:50,518][cascade_classifier.transform] X_groups_test.shape=[(5380, 4)]\n",
      "[ 2019-02-11 15:44:50,521][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 15:44:50,522][cascade_classifier.transform] X_test.shape=(5380, 4)\n",
      "[ 2019-02-11 15:44:50,523][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(5380, 4)\n",
      "[ 2019-02-11 15:44:50,673][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:50,819][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:50,963][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:51,111][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:51,255][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:52,290][cascade_classifier.transform] X_groups_test.shape=[(5380, 4)]\n",
      "[ 2019-02-11 15:44:52,292][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 15:44:52,292][cascade_classifier.transform] X_test.shape=(5380, 4)\n",
      "[ 2019-02-11 15:44:52,294][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(5380, 4)\n",
      "[ 2019-02-11 15:44:52,447][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:52,591][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:52,735][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:52,880][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:53,026][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(5380, 52)\n",
      "[ 2019-02-11 15:44:54,061][cascade_classifier.transform] X_groups_test.shape=[(5377, 4)]\n",
      "[ 2019-02-11 15:44:54,065][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 15:44:54,066][cascade_classifier.transform] X_test.shape=(5377, 4)\n",
      "[ 2019-02-11 15:44:54,067][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(5377, 4)\n",
      "[ 2019-02-11 15:44:54,223][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(5377, 52)\n",
      "[ 2019-02-11 15:44:54,386][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(5377, 52)\n",
      "[ 2019-02-11 15:44:54,552][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(5377, 52)\n",
      "[ 2019-02-11 15:44:54,690][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(5377, 52)\n",
      "[ 2019-02-11 15:44:54,827][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(5377, 52)\n",
      "[ 2019-02-11 15:44:55,870][cascade_classifier.transform] X_groups_test.shape=[(5376, 4)]\n",
      "[ 2019-02-11 15:44:55,872][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 15:44:55,872][cascade_classifier.transform] X_test.shape=(5376, 4)\n",
      "[ 2019-02-11 15:44:55,874][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(5376, 4)\n",
      "[ 2019-02-11 15:44:56,012][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:56,177][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:56,315][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:56,461][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:56,599][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:57,647][cascade_classifier.transform] X_groups_test.shape=[(5376, 4)]\n",
      "[ 2019-02-11 15:44:57,648][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 15:44:57,649][cascade_classifier.transform] X_test.shape=(5376, 4)\n",
      "[ 2019-02-11 15:44:57,650][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(5376, 4)\n",
      "[ 2019-02-11 15:44:57,802][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:57,947][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:58,098][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:58,248][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(5376, 52)\n",
      "[ 2019-02-11 15:44:58,391][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(5376, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE(1-10)  Accuracy: 99.74 $\\pm$ 0.09 %\n"
     ]
    }
   ],
   "source": [
    "population_best_weight = np.load(\"../npy/CE_best_weights(1-10).npy\")\n",
    "\n",
    "classifier_num = 10\n",
    "\n",
    "scores = []\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    K_train_x, K_test_x = X_train[train_index], X_train[test_index]\n",
    "    K_train_y, K_test_y = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    # 所有学习器都输出概率向量，最后投票\n",
    "    y_test_pred_proba_all = []\n",
    "    # 取训练好的模型，计算各模型”验证集“上输出概率向量\n",
    "    for model in models:\n",
    "        model_name = model.__class__.__name__\n",
    "        y_test_pred_proba = model.predict_proba(K_test_x)\n",
    "        y_test_pred_proba_all.append(y_test_pred_proba)\n",
    "\n",
    "    y_test_pred_ensemble_proba = np.zeros((len(K_test_y), 6)) # 初始化集成器概率向量\n",
    "    \n",
    "    # 为每一个基学习器乘上权重\n",
    "    for k in range(classifier_num):\n",
    "        y_test_pred_ensemble_proba += y_test_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_test_pred_ensemble = np.argmax(y_test_pred_ensemble_proba, axis=1)\n",
    "    \n",
    "    scores.append(accuracy_score(K_test_y, y_test_pred_ensemble))\n",
    "print(\"CE(1-10)  Accuracy: %0.2f $\\pm$ %0.2f %%\" % (np.mean(scores)*100, np.std(scores)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gc]",
   "language": "python",
   "name": "conda-env-gc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
