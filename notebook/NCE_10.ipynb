{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda/envs/gc/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "from gcforest.gcforest import GCForest\n",
    "import cmaes as cma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据以及参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其余基本模型的参数都是sklearn默认的\n",
    "\n",
    "设置深度森林参数：最大层数20，连续5层没有效果（WF1）提升则停止，每一层：4个随机森林，3个决策树，1个逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_toy_config():\n",
    "    config = {}\n",
    "    ca_config = {}\n",
    "    ca_config[\"random_state\"] = random_seed\n",
    "    ca_config[\"max_layers\"] = 20\n",
    "    ca_config[\"early_stopping_rounds\"] = 5\n",
    "    ca_config[\"n_classes\"] = 6\n",
    "    ca_config[\"estimators\"] = []\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"RandomForestClassifier\", \"random_state\" : random_seed})\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"RandomForestClassifier\", \"random_state\" : random_seed})\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"RandomForestClassifier\", \"random_state\" : random_seed})\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"RandomForestClassifier\", \"random_state\" : random_seed})\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"DecisionTreeClassifier\"})\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"DecisionTreeClassifier\"})\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"DecisionTreeClassifier\"})\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"LogisticRegression\"})\n",
    "    config[\"cascade\"] = ca_config\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()+'/../data/20122018freshwater_four_feature.csv'\n",
    "data = pd.read_csv(path, na_values = np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training/valid/test: 0.6/0.2/0.2, 各数据集划分的时候要注意\n",
    "X = data.drop(['本周水质'], axis=1).values # Series\n",
    "y = data['本周水质'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                       stratify = y, random_state = random_seed)\n",
    "# Z-score\n",
    "clean_pipeline = Pipeline([('imputer', preprocessing.Imputer(missing_values='NaN',strategy=\"median\")),\n",
    "                           ('std_scaler', preprocessing.StandardScaler()),])\n",
    "X_train = clean_pipeline.fit_transform(X_train)\n",
    "X_test = clean_pipeline.fit_transform(X_test)\n",
    "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, test_size=0.25, \n",
    "                                       stratify = y_train, random_state = random_seed)\n",
    "X_train = X_train2\n",
    "y_train = y_train2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Pre-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = get_toy_config()\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    SVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    ExtraTreeClassifier(),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(),\n",
    "    RandomForestClassifier(random_state=random_seed),\n",
    "    ExtraTreesClassifier(random_state=random_seed),\n",
    "    GCForest(config)\n",
    "]\n",
    "\n",
    "\n",
    "# # 除去所有表现优异的树模型，保留3个基础的机器学习模型\n",
    "# models = [\n",
    "#     LogisticRegression(),\n",
    "#     LinearDiscriminantAnalysis(),\n",
    "#     SVC(probability=True),\n",
    "#     GaussianNB(),\n",
    "#     KNeighborsClassifier(),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda/envs/gc/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba_all = [] # 各模型权重参数\n",
    "\n",
    "test_entries = [] # 各基模型测试集结果\n",
    "train_entries = [] # 各基模型训练集结果\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    if model_name == 'GCForest':\n",
    "        model.fit_transform(X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_valid) # 20%验证集上的概率向量，为训练权重做准备\n",
    "    \n",
    "    f1_train = f1_score(y_train, y_train_pred, average='weighted')\n",
    "    f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    acc_train = accuracy_score(y_train, y_train_pred)\n",
    "    acc_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    train_entries.append((model_name, f1_train, acc_train))\n",
    "    test_entries.append((model_name, f1_test, acc_test))\n",
    "    y_pred_proba_all.append(y_pred_proba)\n",
    "    \n",
    "train_df = pd.DataFrame(train_entries, columns=['model_name', 'train_f1_weighted', 'train_accuracy'])\n",
    "test_df = pd.DataFrame(test_entries, columns=['model_name', 'test_f1_weighted', 'test_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Results on training data\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Results on test data\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度森林训练结果：4层不再有效果提升\n",
    "\n",
    "opt_layer_num=4, weighted_f1_train=99.45%, weighted_f1_test=97.66%\n",
    "\n",
    "60%训练集，20%的测试集仅用做展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_proba_all = np.asarray(y_pred_proba_all)\n",
    "np.save(\"../npy/NCE_proba_of_10models.npy\", y_pred_proba_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 直接载入各个模型在验证集的概率向量，仅在懒得pre train的时候使用\n",
    "# y_pred_proba_all = np.load(\"../npy/NCE_proba_of_10models.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train weights via NCE Ensemble on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNCE(X_valid, y_valid, y_pred_proba_all):\n",
    "    number_of_exp = 100\n",
    "    classifier_num = 10\n",
    "    population_num = 1000\n",
    "    retain_population_num = 100\n",
    "    max_iteration = 50\n",
    "    population_weights = np.zeros((population_num, classifier_num))\n",
    "    population_retain_weights = np.zeros((retain_population_num, classifier_num))\n",
    "    population_score = []\n",
    "    population_retain_score = []\n",
    "\n",
    "    all_best_weights = np.zeros((max_iteration, classifier_num)) # 某次训练时，所有迭代步骤中最好的种群的权重\n",
    "    all_best_f1s = np.zeros(max_iteration) # 某次训练时，每次迭代都取精英种群中最高的f1，构成这个“最高f1数组”\n",
    "    all_mean_f1s = np.zeros(max_iteration) # 某次训练时，每次迭代都取精英种群f1的均值，构成这个“平均f1数组”\n",
    "    all_best_f1s_mean = np.zeros(number_of_exp) # 每次训练最高f1数组的均值, 即 np.mean(all_best_f1s)\n",
    "    all_best_f1s_std = np.zeros(number_of_exp) # 每次训练最高f1数组的标准差, 即 np.std(all_best_f1s)\n",
    "    all_mean_f1s_mean = np.zeros(number_of_exp)\n",
    "    all_mean_f1s_std = np.zeros(number_of_exp)\n",
    "\n",
    "    mu = np.zeros(classifier_num)\n",
    "    sigma = np.ones(classifier_num)\n",
    "\n",
    "\n",
    "    # 在验证集集上: 训练每个基学习器的投票参数\n",
    "    for i in range(max_iteration):\n",
    "        print(\"Iteration: %d\" %(i))\n",
    "        # 该次迭代的所有种群们\n",
    "        population_score = np.zeros(population_num)\n",
    "        population_weights = np.zeros((population_num, classifier_num))\n",
    "        # 该次迭代的优势种群们\n",
    "        population_retain_score = np.zeros(retain_population_num)\n",
    "        population_retain_weights = np.zeros((retain_population_num, classifier_num))\n",
    "\n",
    "        # 生成所有种群\n",
    "        for j in range(classifier_num):\n",
    "            w = np.random.normal(mu[j], sigma[j]+700/(i+1), population_num)\n",
    "            # w = np.random.normal(mu[j], sigma[j], population_num)\n",
    "            population_weights[:,j] = w\n",
    "\n",
    "        # 映射所有种群的权重至[0:1]\n",
    "        for j in range(population_num):\n",
    "            w2 = np.zeros(classifier_num)\n",
    "            for k in range(classifier_num):\n",
    "                w2[k] = np.exp(-population_weights[j][k]*population_weights[j][k])\n",
    "                # w2[k] = np.exp(population_weights[j][k])/np.sum(np.exp(population_weights[j]))\n",
    "            population_weights[j] = w2\n",
    "\n",
    "        # 计算所有种群得分\n",
    "        for j in range(population_num):\n",
    "            y_pred_ensemble_proba = np.zeros((len(y_valid), 6)) # 集成器概率向量\n",
    "            # 为每一个基学习器乘上权重\n",
    "            for k in range(classifier_num):\n",
    "                y_pred_ensemble_proba += y_pred_proba_all[k] * population_weights[j][k]\n",
    "            y_pred_ensemble = np.argmax(y_pred_ensemble_proba, axis=1)\n",
    "            f1 = f1_score(y_valid, y_pred_ensemble, average=\"weighted\")\n",
    "            population_score[j] = f1\n",
    "\n",
    "        # 所有种群得分按降序排列\n",
    "        retain_index = np.argsort(-np.array(population_score))[:retain_population_num]\n",
    "\n",
    "        # 记录该次迭代中的优势种群们\n",
    "        population_retain_weights = population_weights[retain_index] # 精英样本权重\n",
    "        population_retain_score = np.array(population_score)[retain_index] # 精英样本得分\n",
    "\n",
    "        # 记录每次迭代最好的种群和value\n",
    "        all_best_weights[i] = population_retain_weights[0]\n",
    "        all_best_f1s[i] = population_retain_score[0] # 最高得分\n",
    "        all_mean_f1s[i] = np.mean(population_retain_score) # 精英样本平均得分\n",
    "        # 更新mu，sigma为优势种群们的分布\n",
    "        mu = np.mean(population_retain_weights, axis = 0)\n",
    "        sigma = np.std(population_retain_weights, axis = 0) #default: ddof = 0, The divisor used in calculations is N - ddof\n",
    "    #     print(\"mu\\n\",mu)\n",
    "    #     print(\"sigma\\n\", sigma)\n",
    "    #     print(\"Weighted F1 Score after rank\")\n",
    "    #     print(population_retain_score)\n",
    "    #     print(\"Weights\")\n",
    "    #     print(population_retain_weights)\n",
    "    \n",
    "    # 权重训练完毕\n",
    "    print(\"==================Finish training==================\")\n",
    "    last_weight = population_retain_weights[0] # 最后一轮迭代精英样本中具有最高F1的权重\n",
    "    last_f1 = all_best_f1s[-1] # 最后一轮迭代中精英样本中的最高F1\n",
    "    best_f1 = all_best_f1s[np.argmax(all_best_f1s)] # 所有轮迭代中最高的F1\n",
    "    best_weight = all_best_weights[np.argmax(all_best_f1s)] # 所有轮迭代中具有最高F1\n",
    "    \n",
    "    print(\"Best F1 in last iteration: %f\" % (last_f1))\n",
    "    print(\"Last weight in last iteration: %s\" %(last_weight))\n",
    "\n",
    "    print(\"Best F1 in all iterations: %f\" % (best_f1))\n",
    "    print(\"Best weight in all iterations: %s\" %(best_weight))\n",
    "    \n",
    "    print(\"Last mu\\n\", mu)\n",
    "    print(\"Last sigma\\n\", sigma)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(all_best_f1s[:25], 'b', label = 'Best weighted F1 score of elite samples')\n",
    "    plt.plot(all_mean_f1s[:25], 'r', label = 'Mean weighted F1 score of elite samples')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Weighted F1 score')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.grid(True)\n",
    "    # plt.savefig('../img/weighed_F1_iteration(1-10).eps',format='eps')\n",
    "    return last_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train weights via NCE Ensemble on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainCMAES(X_valid, y_valid, y_pred_proba_all):\n",
    "    es = cma.CMAEvolutionStrategy(10 * [0], 0.5)\n",
    "#     help(cma)\n",
    "#     help(es)\n",
    "    score = \"f1_weighted\"\n",
    "#     score = \"accuracy\"\n",
    "    while not es.stop():\n",
    "        solutions = es.ask()\n",
    "        es.tell(solutions, [cma.ff.water_ensemble(x, y_pred_proba_all, y_valid, metric=score) for x in solutions])\n",
    "        es.logger.add()  # write data to disc to be plotted\n",
    "        es.disp()\n",
    "    # 训练完毕\n",
    "    es.result_pretty()  # pretty print result\n",
    "    cma.plot()    \n",
    "    weights = np.exp(es.result.xbest)/np.sum(np.exp(es.result.xbest))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 得到10个基模型的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMAES weights [0.01097652 0.0270836  0.01180409 0.10165539 0.01333095 0.09007401\n",
      " 0.03286971 0.07746972 0.06164148 0.57309454]\n"
     ]
    }
   ],
   "source": [
    "weights = np.exp(es.result.xbest)/np.sum(np.exp(es.result.xbest))\n",
    "# weights = np.load(\"../npy/cmaes_weights.npy\")  # 载入保存的权重，直接载入仅在懒得训练CMAES时使用或者想得到固定的结果\n",
    "\n",
    "# weights of each base models\n",
    "print(\"CMAES weights\", weights)\n",
    "# np.save(\"../npy/cmaes_weights.npy\", weights) # 保存结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代入权重，在3个集合上计算ACC和F1\n",
    "\n",
    "- 训练集（总样本数量的60%）X_train\n",
    "- 验证集（总样本数量的20%，调参数，用CMAES得到权重）X_valid\n",
    "- 测试集（总样本数的20%）X_test\n",
    "- 各模型稳定性基于交叉验证，在除测试集外的80%上做5折 X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================LogisticRegression=================\n",
      "1 train_acc: 3.08\n",
      "2 train_acc: 99.21\n",
      "3 train_acc: 57.22\n",
      "4 train_acc: 45.05\n",
      "5 train_acc: 0.00\n",
      "6 train_acc: 77.02\n",
      "average: 46.93\n",
      "1 valid_acc: 3.07\n",
      "2 valid_acc: 99.13\n",
      "3 valid_acc: 58.50\n",
      "4 valid_acc: 45.21\n",
      "5 valid_acc: 0.00\n",
      "6 valid_acc: 78.79\n",
      "average: 47.45\n",
      "1 test_acc: 5.74\n",
      "2 test_acc: 99.51\n",
      "3 test_acc: 55.17\n",
      "4 test_acc: 48.08\n",
      "5 test_acc: 0.00\n",
      "6 test_acc: 80.19\n",
      "average: 48.11\n",
      "=================LinearDiscriminantAnalysis=================\n",
      "1 train_acc: 0.00\n",
      "2 train_acc: 99.90\n",
      "3 train_acc: 38.08\n",
      "4 train_acc: 50.62\n",
      "5 train_acc: 28.33\n",
      "6 train_acc: 44.57\n",
      "average: 43.58\n",
      "1 valid_acc: 0.00\n",
      "2 valid_acc: 99.85\n",
      "3 valid_acc: 38.49\n",
      "4 valid_acc: 51.42\n",
      "5 valid_acc: 26.94\n",
      "6 valid_acc: 45.45\n",
      "average: 43.69\n",
      "1 test_acc: 0.00\n",
      "2 test_acc: 99.92\n",
      "3 test_acc: 38.58\n",
      "4 test_acc: 52.83\n",
      "5 test_acc: 29.97\n",
      "6 test_acc: 47.32\n",
      "average: 44.77\n",
      "=================SVC=================\n",
      "1 train_acc: 64.64\n",
      "2 train_acc: 93.62\n",
      "3 train_acc: 89.20\n",
      "4 train_acc: 90.07\n",
      "5 train_acc: 87.35\n",
      "6 train_acc: 95.73\n",
      "average: 86.77\n",
      "1 valid_acc: 64.34\n",
      "2 valid_acc: 93.22\n",
      "3 valid_acc: 88.74\n",
      "4 valid_acc: 89.77\n",
      "5 valid_acc: 78.45\n",
      "6 valid_acc: 94.87\n",
      "average: 84.90\n",
      "1 test_acc: 65.16\n",
      "2 test_acc: 93.33\n",
      "3 test_acc: 87.61\n",
      "4 test_acc: 91.77\n",
      "5 test_acc: 87.54\n",
      "6 test_acc: 98.14\n",
      "average: 87.26\n",
      "=================DecisionTreeClassifier=================\n",
      "1 train_acc: 100.00\n",
      "2 train_acc: 100.00\n",
      "3 train_acc: 100.00\n",
      "4 train_acc: 100.00\n",
      "5 train_acc: 100.00\n",
      "6 train_acc: 100.00\n",
      "average: 100.00\n",
      "1 valid_acc: 98.98\n",
      "2 valid_acc: 98.98\n",
      "3 valid_acc: 98.52\n",
      "4 valid_acc: 98.45\n",
      "5 valid_acc: 98.65\n",
      "6 valid_acc: 99.07\n",
      "average: 98.77\n",
      "1 test_acc: 98.98\n",
      "2 test_acc: 97.14\n",
      "3 test_acc: 97.22\n",
      "4 test_acc: 96.71\n",
      "5 test_acc: 91.92\n",
      "6 test_acc: 98.83\n",
      "average: 96.80\n",
      "=================ExtraTreeClassifier=================\n",
      "1 train_acc: 100.00\n",
      "2 train_acc: 100.00\n",
      "3 train_acc: 100.00\n",
      "4 train_acc: 100.00\n",
      "5 train_acc: 100.00\n",
      "6 train_acc: 100.00\n",
      "average: 100.00\n",
      "1 valid_acc: 90.78\n",
      "2 valid_acc: 94.69\n",
      "3 valid_acc: 92.27\n",
      "4 valid_acc: 88.58\n",
      "5 valid_acc: 65.32\n",
      "6 valid_acc: 86.71\n",
      "average: 86.39\n",
      "1 test_acc: 89.96\n",
      "2 test_acc: 93.41\n",
      "3 test_acc: 91.02\n",
      "4 test_acc: 86.56\n",
      "5 test_acc: 73.06\n",
      "6 test_acc: 88.11\n",
      "average: 87.02\n",
      "=================GaussianNB=================\n",
      "1 train_acc: 78.32\n",
      "2 train_acc: 90.94\n",
      "3 train_acc: 82.30\n",
      "4 train_acc: 69.57\n",
      "5 train_acc: 60.13\n",
      "6 train_acc: 67.70\n",
      "average: 74.83\n",
      "1 valid_acc: 80.53\n",
      "2 valid_acc: 90.47\n",
      "3 valid_acc: 81.92\n",
      "4 valid_acc: 70.05\n",
      "5 valid_acc: 56.23\n",
      "6 valid_acc: 71.33\n",
      "average: 75.09\n",
      "1 test_acc: 79.51\n",
      "2 test_acc: 90.81\n",
      "3 test_acc: 84.03\n",
      "4 test_acc: 71.30\n",
      "5 test_acc: 55.22\n",
      "6 test_acc: 74.13\n",
      "average: 75.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-02-25 20:32:41,766][cascade_classifier.transform] X_groups_test.shape=[(20166, 4)]\n",
      "[ 2019-02-25 20:32:41,768][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-25 20:32:41,769][cascade_classifier.transform] X_test.shape=(20166, 4)\n",
      "[ 2019-02-25 20:32:41,771][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(20166, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================KNeighborsClassifier=================\n",
      "1 train_acc: 84.06\n",
      "2 train_acc: 96.51\n",
      "3 train_acc: 93.43\n",
      "4 train_acc: 94.21\n",
      "5 train_acc: 85.89\n",
      "6 train_acc: 94.88\n",
      "average: 91.50\n",
      "1 valid_acc: 77.46\n",
      "2 valid_acc: 93.48\n",
      "3 valid_acc: 89.31\n",
      "4 valid_acc: 88.68\n",
      "5 valid_acc: 70.03\n",
      "6 valid_acc: 90.44\n",
      "average: 84.90\n",
      "1 test_acc: 76.84\n",
      "2 test_acc: 93.15\n",
      "3 test_acc: 88.75\n",
      "4 test_acc: 89.58\n",
      "5 test_acc: 81.82\n",
      "6 test_acc: 93.94\n",
      "average: 87.35\n",
      "=================RandomForestClassifier=================\n",
      "1 train_acc: 99.66\n",
      "2 train_acc: 99.97\n",
      "3 train_acc: 99.94\n",
      "4 train_acc: 99.85\n",
      "5 train_acc: 99.66\n",
      "6 train_acc: 99.92\n",
      "average: 99.84\n",
      "1 valid_acc: 99.18\n",
      "2 valid_acc: 99.51\n",
      "3 valid_acc: 99.55\n",
      "4 valid_acc: 99.27\n",
      "5 valid_acc: 98.65\n",
      "6 valid_acc: 99.30\n",
      "average: 99.24\n",
      "1 test_acc: 99.39\n",
      "2 test_acc: 97.55\n",
      "3 test_acc: 98.01\n",
      "4 test_acc: 96.98\n",
      "5 test_acc: 93.27\n",
      "6 test_acc: 98.83\n",
      "average: 97.34\n",
      "=================ExtraTreesClassifier=================\n",
      "1 train_acc: 100.00\n",
      "2 train_acc: 100.00\n",
      "3 train_acc: 100.00\n",
      "4 train_acc: 100.00\n",
      "5 train_acc: 100.00\n",
      "6 train_acc: 100.00\n",
      "average: 100.00\n",
      "1 valid_acc: 98.36\n",
      "2 valid_acc: 98.64\n",
      "3 valid_acc: 97.56\n",
      "4 valid_acc: 96.35\n",
      "5 valid_acc: 87.88\n",
      "6 valid_acc: 96.27\n",
      "average: 95.84\n",
      "1 test_acc: 98.36\n",
      "2 test_acc: 97.21\n",
      "3 test_acc: 95.28\n",
      "4 test_acc: 94.97\n",
      "5 test_acc: 91.58\n",
      "6 test_acc: 98.37\n",
      "average: 95.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-02-25 20:32:42,244][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-25 20:32:42,714][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-25 20:32:42,715][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-25 20:32:42,716][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:32:42,717][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:32:42,875][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-25 20:32:43,047][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-25 20:32:43,048][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-25 20:32:43,049][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:32:43,050][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:32:43,208][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================GCForest=================\n",
      "1 train_acc: 99.52\n",
      "2 train_acc: 99.90\n",
      "3 train_acc: 99.89\n",
      "4 train_acc: 100.00\n",
      "5 train_acc: 99.66\n",
      "6 train_acc: 99.77\n",
      "average: 99.79\n",
      "1 valid_acc: 99.39\n",
      "2 valid_acc: 99.51\n",
      "3 valid_acc: 99.55\n",
      "4 valid_acc: 99.27\n",
      "5 valid_acc: 99.33\n",
      "6 valid_acc: 99.07\n",
      "average: 99.35\n",
      "1 test_acc: 99.39\n",
      "2 test_acc: 97.59\n",
      "3 test_acc: 97.95\n",
      "4 test_acc: 96.98\n",
      "5 test_acc: 93.27\n",
      "6 test_acc: 99.07\n",
      "average: 97.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-02-25 20:32:48,946][cascade_classifier.transform] X_groups_test.shape=[(20166, 4)]\n",
      "[ 2019-02-25 20:32:48,947][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-25 20:32:48,948][cascade_classifier.transform] X_test.shape=(20166, 4)\n",
      "[ 2019-02-25 20:32:48,949][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(20166, 4)\n",
      "[ 2019-02-25 20:32:49,357][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-25 20:32:49,813][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-25 20:32:49,814][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-25 20:32:49,815][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:32:49,816][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:32:49,977][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-25 20:32:50,151][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-25 20:32:50,152][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-25 20:32:50,153][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:32:50,154][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:32:50,309][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================CMAES=================\n",
      "1 train_acc: 99.66\n",
      "2 train_acc: 99.99\n",
      "3 train_acc: 99.96\n",
      "4 train_acc: 100.00\n",
      "5 train_acc: 99.89\n",
      "6 train_acc: 100.00\n",
      "average: 99.92\n",
      "1 valid_acc: 99.39\n",
      "2 valid_acc: 99.51\n",
      "3 valid_acc: 99.55\n",
      "4 valid_acc: 99.27\n",
      "5 valid_acc: 99.33\n",
      "6 valid_acc: 99.07\n",
      "average: 99.35\n",
      "1 test_acc: 99.39\n",
      "2 test_acc: 97.59\n",
      "3 test_acc: 97.95\n",
      "4 test_acc: 96.98\n",
      "5 test_acc: 93.27\n",
      "6 test_acc: 99.07\n",
      "average: 97.37\n"
     ]
    }
   ],
   "source": [
    "# 取训练好的模型\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    train_pred = model.predict(X_train)\n",
    "    valid_pred = model.predict(X_valid)\n",
    "    test_pred = model.predict(X_test)\n",
    "    print(\"=================\" + model_name + \"=================\")\n",
    "    train_cm = confusion_matrix(y_train, train_pred)\n",
    "    valid_cm = confusion_matrix(y_valid, valid_pred)\n",
    "    test_cm = confusion_matrix(y_test, test_pred)\n",
    "    i=0\n",
    "    train_acc_all = np.zeros(6)\n",
    "    for c in train_cm:\n",
    "        train_acc_all[i] = c[i]/np.sum(c)\n",
    "        print(\"%d train_acc: %.2f\" %(i+1, 100*train_acc_all[i]))\n",
    "        i=i+1\n",
    "    print(\"average: %.2f\" % (100*np.mean(train_acc_all)))\n",
    "    i=0\n",
    "    valid_acc_all = np.zeros(6)\n",
    "    for c in valid_cm:\n",
    "        valid_acc_all[i] = c[i]/np.sum(c)\n",
    "        print(\"%d valid_acc: %.2f\" %(i+1, 100*valid_acc_all[i]))\n",
    "        i=i+1\n",
    "    print(\"average: %.2f\" % (100*np.mean(valid_acc_all)))\n",
    "    i=0\n",
    "    test_acc_all = np.zeros(6)\n",
    "    for c in test_cm:\n",
    "        test_acc_all[i] = c[i]/np.sum(c)\n",
    "        print(\"%d test_acc: %.2f\" %(i+1, 100*test_acc_all[i]))\n",
    "        i=i+1\n",
    "    print(\"average: %.2f\" % (100*np.mean(test_acc_all)))\n",
    "\n",
    "\n",
    "# - 计算CE（1-10）\n",
    "\n",
    "population_best_weight = weights\n",
    "\n",
    "classifier_num = 10\n",
    "\n",
    "# 所有学习器都输出概率向量，最后投票\n",
    "y_train_pred_proba_all = []\n",
    "y_valid_pred_proba_all = []\n",
    "y_test_pred_proba_all = []\n",
    "\n",
    "# 取训练好的模型，计算各模型”验证集“上输出概率向量\n",
    "for model in models:\n",
    "    train_pred_proba = model.predict_proba(X_train)\n",
    "    valid_pred_proba = model.predict_proba(X_valid)\n",
    "    test_pred_proba = model.predict_proba(X_test)\n",
    "    y_train_pred_proba_all.append(train_pred_proba)\n",
    "    y_valid_pred_proba_all.append(valid_pred_proba)\n",
    "    y_test_pred_proba_all.append(test_pred_proba)\n",
    "    \n",
    "y_train_pred_ensemble_proba = np.zeros((len(y_train), 6)) # 初始化集成器概率向量\n",
    "y_valid_pred_ensemble_proba = np.zeros((len(y_valid), 6)) # 初始化集成器概率向量\n",
    "y_test_pred_ensemble_proba = np.zeros((len(y_test), 6)) # 初始化集成器概率向量\n",
    "\n",
    "# 为每一个基学习器乘上权重\n",
    "for k in range(classifier_num):\n",
    "    y_train_pred_ensemble_proba += y_train_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_valid_pred_ensemble_proba += y_valid_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_test_pred_ensemble_proba += y_test_pred_proba_all[k] * population_best_weight[k]\n",
    "y_train_pred_ensemble = np.argmax(y_train_pred_ensemble_proba, axis=1)\n",
    "y_valid_pred_ensemble = np.argmax(y_valid_pred_ensemble_proba, axis=1)\n",
    "y_test_pred_ensemble = np.argmax(y_test_pred_ensemble_proba, axis=1)\n",
    "\n",
    "# 计算各水质等级的得分\n",
    "print(\"=================CMAES=================\")\n",
    "train_cm = confusion_matrix(y_train, y_train_pred_ensemble)\n",
    "valid_cm = confusion_matrix(y_valid, y_valid_pred_ensemble)\n",
    "test_cm = confusion_matrix(y_test, y_test_pred_ensemble)\n",
    "i=0\n",
    "train_acc_all = np.zeros(6)\n",
    "for c in train_cm:\n",
    "    train_acc_all[i] = c[i]/np.sum(c)\n",
    "    print(\"%d train_acc: %.2f\" %(i+1, 100*train_acc_all[i]))\n",
    "    i=i+1\n",
    "print(\"average: %.2f\" % (100*np.mean(train_acc_all)))\n",
    "i=0\n",
    "valid_acc_all = np.zeros(6)\n",
    "for c in valid_cm:\n",
    "    valid_acc_all[i] = c[i]/np.sum(c)\n",
    "    print(\"%d valid_acc: %.2f\" %(i+1, 100*valid_acc_all[i]))\n",
    "    i=i+1\n",
    "print(\"average: %.2f\" % (100*np.mean(valid_acc_all)))\n",
    "i=0\n",
    "test_acc_all = np.zeros(6)\n",
    "for c in test_cm:\n",
    "    test_acc_all[i] = c[i]/np.sum(c)\n",
    "    print(\"%d test_acc: %.2f\" %(i+1, 100*test_acc_all[i]))\n",
    "    i=i+1\n",
    "print(\"average: %.2f\" % (100*np.mean(test_acc_all)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================LogisticRegression=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8654    0.0308    0.0594      1462\n",
      "          1     0.6703    0.9921    0.8000      7962\n",
      "          2     0.6432    0.5722    0.6056      5278\n",
      "          3     0.6217    0.4505    0.5224      3283\n",
      "          4     0.0000    0.0000    0.0000       893\n",
      "          5     0.7904    0.7702    0.7802      1288\n",
      "\n",
      "avg / total     0.6474    0.6662    0.6136     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9375    0.0307    0.0595       488\n",
      "          1     0.6764    0.9913    0.8042      2655\n",
      "          2     0.6439    0.5850    0.6130      1759\n",
      "          3     0.6338    0.4521    0.5277      1095\n",
      "          4     0.0000    0.0000    0.0000       297\n",
      "          5     0.7735    0.7879    0.7806       429\n",
      "\n",
      "avg / total     0.6562    0.6707    0.6181      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9333    0.0574    0.1081       488\n",
      "          1     0.6672    0.9951    0.7988      2655\n",
      "          2     0.6628    0.5517    0.6022      1760\n",
      "          3     0.6360    0.4808    0.5476      1094\n",
      "          4     0.0000    0.0000    0.0000       297\n",
      "          5     0.7818    0.8019    0.7917       429\n",
      "\n",
      "avg / total     0.6581    0.6710    0.6206      6723\n",
      "\n",
      "=================LinearDiscriminantAnalysis=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.0000    0.0000    0.0000      1462\n",
      "          1     0.5995    0.9990    0.7494      7962\n",
      "          2     0.5908    0.3808    0.4631      5278\n",
      "          3     0.6896    0.5062    0.5839      3283\n",
      "          4     0.5030    0.2833    0.3625       893\n",
      "          5     0.9863    0.4457    0.6139      1288\n",
      "\n",
      "avg / total     0.5889    0.6175    0.5674     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.0000    0.0000    0.0000       488\n",
      "          1     0.6000    0.9985    0.7496      2655\n",
      "          2     0.5918    0.3849    0.4664      1759\n",
      "          3     0.7011    0.5142    0.5933      1095\n",
      "          4     0.5031    0.2694    0.3509       297\n",
      "          5     0.9848    0.4545    0.6220       429\n",
      "\n",
      "avg / total     0.5911    0.6197    0.5699      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.0000    0.0000    0.0000       488\n",
      "          1     0.6016    0.9992    0.7510      2655\n",
      "          2     0.6036    0.3858    0.4707      1760\n",
      "          3     0.7075    0.5283    0.6049      1094\n",
      "          4     0.5361    0.2997    0.3844       297\n",
      "          5     0.9902    0.4732    0.6404       429\n",
      "\n",
      "avg / total     0.5976    0.6250    0.5761      6723\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda/envs/gc/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================SVC=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8607    0.6464    0.7383      1462\n",
      "          1     0.8850    0.9362    0.9099      7962\n",
      "          2     0.8812    0.8920    0.8865      5278\n",
      "          3     0.9375    0.9007    0.9188      3283\n",
      "          4     0.8914    0.8735    0.8824       893\n",
      "          5     0.9686    0.9573    0.9629      1288\n",
      "\n",
      "avg / total     0.8964    0.8964    0.8949     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8509    0.6434    0.7328       488\n",
      "          1     0.8802    0.9322    0.9054      2655\n",
      "          2     0.8760    0.8874    0.8817      1759\n",
      "          3     0.9239    0.8977    0.9106      1095\n",
      "          4     0.8826    0.7845    0.8307       297\n",
      "          5     0.9421    0.9487    0.9454       429\n",
      "\n",
      "avg / total     0.8881    0.8884    0.8868      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7970    0.6516    0.7170       488\n",
      "          1     0.8790    0.9333    0.9054      2655\n",
      "          2     0.9012    0.8761    0.8885      1760\n",
      "          3     0.9366    0.9177    0.9271      1094\n",
      "          4     0.9253    0.8754    0.8997       297\n",
      "          5     0.9546    0.9814    0.9678       429\n",
      "\n",
      "avg / total     0.8951    0.8959    0.8945      6723\n",
      "\n",
      "=================DecisionTreeClassifier=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    1.0000    1.0000      1462\n",
      "          1     1.0000    1.0000    1.0000      7962\n",
      "          2     1.0000    1.0000    1.0000      5278\n",
      "          3     1.0000    1.0000    1.0000      3283\n",
      "          4     1.0000    1.0000    1.0000       893\n",
      "          5     1.0000    1.0000    1.0000      1288\n",
      "\n",
      "avg / total     1.0000    1.0000    1.0000     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9817    0.9898    0.9857       488\n",
      "          1     0.9898    0.9898    0.9898      2655\n",
      "          2     0.9858    0.9852    0.9855      1759\n",
      "          3     0.9926    0.9845    0.9885      1095\n",
      "          4     0.9702    0.9865    0.9783       297\n",
      "          5     0.9884    0.9907    0.9895       429\n",
      "\n",
      "avg / total     0.9877    0.9877    0.9877      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8961    0.9898    0.9406       488\n",
      "          1     0.9874    0.9714    0.9793      2655\n",
      "          2     0.9800    0.9722    0.9760      1760\n",
      "          3     0.9742    0.9671    0.9706      1094\n",
      "          4     0.9254    0.9192    0.9223       297\n",
      "          5     0.9528    0.9883    0.9703       429\n",
      "\n",
      "avg / total     0.9717    0.9710    0.9711      6723\n",
      "\n",
      "=================ExtraTreeClassifier=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    1.0000    1.0000      1462\n",
      "          1     1.0000    1.0000    1.0000      7962\n",
      "          2     1.0000    1.0000    1.0000      5278\n",
      "          3     1.0000    1.0000    1.0000      3283\n",
      "          4     1.0000    1.0000    1.0000       893\n",
      "          5     1.0000    1.0000    1.0000      1288\n",
      "\n",
      "avg / total     1.0000    1.0000    1.0000     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8569    0.9078    0.8816       488\n",
      "          1     0.9603    0.9469    0.9535      2655\n",
      "          2     0.9164    0.9227    0.9195      1759\n",
      "          3     0.8661    0.8858    0.8758      1095\n",
      "          4     0.7055    0.6532    0.6783       297\n",
      "          5     0.8815    0.8671    0.8743       429\n",
      "\n",
      "avg / total     0.9097    0.9097    0.9096      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8085    0.8996    0.8516       488\n",
      "          1     0.9549    0.9341    0.9444      2655\n",
      "          2     0.9118    0.9102    0.9110      1760\n",
      "          3     0.8532    0.8656    0.8593      1094\n",
      "          4     0.7115    0.7306    0.7209       297\n",
      "          5     0.9197    0.8811    0.9000       429\n",
      "\n",
      "avg / total     0.9034    0.9018    0.9024      6723\n",
      "\n",
      "=================GaussianNB=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8407    0.7832    0.8109      1462\n",
      "          1     0.8573    0.9094    0.8826      7962\n",
      "          2     0.7624    0.8230    0.7915      5278\n",
      "          3     0.7706    0.6957    0.7312      3283\n",
      "          4     0.6671    0.6013    0.6325       893\n",
      "          5     0.9787    0.6770    0.8004      1288\n",
      "\n",
      "avg / total     0.8165    0.8144    0.8126     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8256    0.8053    0.8154       488\n",
      "          1     0.8606    0.9047    0.8821      2655\n",
      "          2     0.7600    0.8192    0.7885      1759\n",
      "          3     0.7755    0.7005    0.7361      1095\n",
      "          4     0.6448    0.5623    0.6007       297\n",
      "          5     0.9808    0.7133    0.8259       429\n",
      "\n",
      "avg / total     0.8160    0.8145    0.8130      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7886    0.7951    0.7918       488\n",
      "          1     0.8657    0.9081    0.8864      2655\n",
      "          2     0.7909    0.8403    0.8149      1760\n",
      "          3     0.7816    0.7130    0.7457      1094\n",
      "          4     0.6560    0.5522    0.5996       297\n",
      "          5     0.9695    0.7413    0.8402       429\n",
      "\n",
      "avg / total     0.8242    0.8240    0.8223      6723\n",
      "\n",
      "=================KNeighborsClassifier=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8741    0.8406    0.8570      1462\n",
      "          1     0.9355    0.9651    0.9500      7962\n",
      "          2     0.9457    0.9343    0.9400      5278\n",
      "          3     0.9520    0.9421    0.9470      3283\n",
      "          4     0.9208    0.8589    0.8888       893\n",
      "          5     0.9776    0.9488    0.9630      1288\n",
      "\n",
      "avg / total     0.9384    0.9385    0.9383     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7908    0.7746    0.7826       488\n",
      "          1     0.8996    0.9348    0.9169      2655\n",
      "          2     0.9013    0.8931    0.8972      1759\n",
      "          3     0.9092    0.8868    0.8978      1095\n",
      "          4     0.7879    0.7003    0.7415       297\n",
      "          5     0.9440    0.9044    0.9238       429\n",
      "\n",
      "avg / total     0.8916    0.8922    0.8916      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7530    0.7684    0.7606       488\n",
      "          1     0.9016    0.9315    0.9163      2655\n",
      "          2     0.9135    0.8875    0.9003      1760\n",
      "          3     0.9245    0.8958    0.9099      1094\n",
      "          4     0.8467    0.8182    0.8322       297\n",
      "          5     0.9482    0.9394    0.9438       429\n",
      "\n",
      "avg / total     0.8982    0.8978    0.8978      6723\n",
      "\n",
      "=================RandomForestClassifier=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9993    0.9966    0.9979      1462\n",
      "          1     0.9990    0.9997    0.9994      7962\n",
      "          2     0.9989    0.9994    0.9991      5278\n",
      "          3     0.9997    0.9985    0.9991      3283\n",
      "          4     0.9989    0.9966    0.9978       893\n",
      "          5     0.9984    0.9992    0.9988      1288\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9991     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9898    0.9918    0.9908       488\n",
      "          1     0.9966    0.9951    0.9959      2655\n",
      "          2     0.9938    0.9955    0.9946      1759\n",
      "          3     0.9945    0.9927    0.9936      1095\n",
      "          4     0.9865    0.9865    0.9865       297\n",
      "          5     0.9884    0.9930    0.9907       429\n",
      "\n",
      "avg / total     0.9941    0.9941    0.9941      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9065    0.9939    0.9482       488\n",
      "          1     0.9927    0.9755    0.9840      2655\n",
      "          2     0.9835    0.9801    0.9818      1760\n",
      "          3     0.9779    0.9698    0.9738      1094\n",
      "          4     0.9295    0.9327    0.9311       297\n",
      "          5     0.9593    0.9883    0.9736       429\n",
      "\n",
      "avg / total     0.9767    0.9761    0.9762      6723\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-02-25 20:33:02,139][cascade_classifier.transform] X_groups_test.shape=[(20166, 4)]\n",
      "[ 2019-02-25 20:33:02,142][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-25 20:33:02,143][cascade_classifier.transform] X_test.shape=(20166, 4)\n",
      "[ 2019-02-25 20:33:02,146][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(20166, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================ExtraTreesClassifier=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    1.0000    1.0000      1462\n",
      "          1     1.0000    1.0000    1.0000      7962\n",
      "          2     1.0000    1.0000    1.0000      5278\n",
      "          3     1.0000    1.0000    1.0000      3283\n",
      "          4     1.0000    1.0000    1.0000       893\n",
      "          5     1.0000    1.0000    1.0000      1288\n",
      "\n",
      "avg / total     1.0000    1.0000    1.0000     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9619    0.9836    0.9726       488\n",
      "          1     0.9835    0.9864    0.9850      2655\n",
      "          2     0.9717    0.9756    0.9736      1759\n",
      "          3     0.9697    0.9635    0.9666      1095\n",
      "          4     0.9321    0.8788    0.9047       297\n",
      "          5     0.9672    0.9627    0.9650       429\n",
      "\n",
      "avg / total     0.9733    0.9734    0.9733      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8905    0.9836    0.9348       488\n",
      "          1     0.9803    0.9721    0.9762      2655\n",
      "          2     0.9710    0.9528    0.9619      1760\n",
      "          3     0.9506    0.9497    0.9502      1094\n",
      "          4     0.9189    0.9158    0.9174       297\n",
      "          5     0.9701    0.9837    0.9769       429\n",
      "\n",
      "avg / total     0.9631    0.9625    0.9626      6723\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-02-25 20:33:02,607][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-25 20:33:03,323][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-25 20:33:03,324][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-25 20:33:03,325][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:33:03,326][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:33:03,491][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-25 20:33:03,671][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-25 20:33:03,672][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-25 20:33:03,673][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:33:03,674][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:33:03,832][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-25 20:33:04,006][cascade_classifier.transform] X_groups_test.shape=[(20166, 4)]\n",
      "[ 2019-02-25 20:33:04,007][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-25 20:33:04,008][cascade_classifier.transform] X_test.shape=(20166, 4)\n",
      "[ 2019-02-25 20:33:04,010][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(20166, 4)\n",
      "[ 2019-02-25 20:33:04,410][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-25 20:33:04,882][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-25 20:33:04,883][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-25 20:33:04,884][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:33:04,885][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:33:05,036][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-25 20:33:05,308][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-25 20:33:05,310][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-25 20:33:05,310][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:33:05,312][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-25 20:33:05,494][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================GCForest=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9979    0.9952    0.9966      1462\n",
      "          1     0.9987    0.9990    0.9989      7962\n",
      "          2     0.9991    0.9989    0.9990      5278\n",
      "          3     0.9982    1.0000    0.9991      3283\n",
      "          4     0.9978    0.9966    0.9972       893\n",
      "          5     0.9992    0.9977    0.9984      1288\n",
      "\n",
      "avg / total     0.9987    0.9987    0.9987     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9898    0.9939    0.9918       488\n",
      "          1     0.9970    0.9951    0.9960      2655\n",
      "          2     0.9938    0.9955    0.9946      1759\n",
      "          3     0.9945    0.9927    0.9936      1095\n",
      "          4     0.9833    0.9933    0.9883       297\n",
      "          5     0.9930    0.9907    0.9918       429\n",
      "\n",
      "avg / total     0.9944    0.9943    0.9943      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9082    0.9939    0.9491       488\n",
      "          1     0.9923    0.9759    0.9840      2655\n",
      "          2     0.9835    0.9795    0.9815      1760\n",
      "          3     0.9779    0.9698    0.9738      1094\n",
      "          4     0.9327    0.9327    0.9327       297\n",
      "          5     0.9594    0.9907    0.9748       429\n",
      "\n",
      "avg / total     0.9768    0.9762    0.9763      6723\n",
      "\n",
      "=================CMAES=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    0.9966    0.9983      1462\n",
      "          1     0.9992    0.9999    0.9996      7962\n",
      "          2     0.9998    0.9996    0.9997      5278\n",
      "          3     0.9994    1.0000    0.9997      3283\n",
      "          4     1.0000    0.9989    0.9994       893\n",
      "          5     1.0000    1.0000    1.0000      1288\n",
      "\n",
      "avg / total     0.9996    0.9996    0.9996     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9898    0.9939    0.9918       488\n",
      "          1     0.9970    0.9951    0.9960      2655\n",
      "          2     0.9938    0.9955    0.9946      1759\n",
      "          3     0.9945    0.9927    0.9936      1095\n",
      "          4     0.9833    0.9933    0.9883       297\n",
      "          5     0.9930    0.9907    0.9918       429\n",
      "\n",
      "avg / total     0.9944    0.9943    0.9943      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9082    0.9939    0.9491       488\n",
      "          1     0.9923    0.9759    0.9840      2655\n",
      "          2     0.9835    0.9795    0.9815      1760\n",
      "          3     0.9779    0.9698    0.9738      1094\n",
      "          4     0.9327    0.9327    0.9327       297\n",
      "          5     0.9594    0.9907    0.9748       429\n",
      "\n",
      "avg / total     0.9768    0.9762    0.9763      6723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 所有学习器都输出概率向量，最后投票\n",
    "y_train_pred_proba_all = []\n",
    "y_valid_pred_proba_all = []\n",
    "y_test_pred_proba_all = []\n",
    "\n",
    "# 取训练好的模型，计算各模型”验证集“上输出概率向量\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    train_pred_proba = model.predict_proba(X_train)\n",
    "    valid_pred_proba = model.predict_proba(X_valid)\n",
    "    test_pred_proba = model.predict_proba(X_test)\n",
    "    train_pred = model.predict(X_train)\n",
    "    valid_pred = model.predict(X_valid)\n",
    "    test_pred = model.predict(X_test)\n",
    "    print(\"=================\" + model_name + \"=================\")\n",
    "    print(classification_report(y_train, train_pred, digits=4))\n",
    "    print(classification_report(y_valid, valid_pred, digits=4))\n",
    "    print(classification_report(y_test, test_pred, digits=4))\n",
    "\n",
    "    y_train_pred_proba_all.append(train_pred_proba)\n",
    "    y_valid_pred_proba_all.append(valid_pred_proba)\n",
    "    y_test_pred_proba_all.append(test_pred_proba)\n",
    "\n",
    "    \n",
    "y_train_pred_ensemble_proba = np.zeros((len(y_train), 6)) # 初始化集成器概率向量\n",
    "y_valid_pred_ensemble_proba = np.zeros((len(y_valid), 6)) # 初始化集成器概率向量\n",
    "y_test_pred_ensemble_proba = np.zeros((len(y_test), 6)) # 初始化集成器概率向量\n",
    "\n",
    "# 为每一个基学习器乘上权重\n",
    "for k in range(classifier_num):\n",
    "    y_train_pred_ensemble_proba += y_train_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_valid_pred_ensemble_proba += y_valid_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_test_pred_ensemble_proba += y_test_pred_proba_all[k] * population_best_weight[k]\n",
    "y_train_pred_ensemble = np.argmax(y_train_pred_ensemble_proba, axis=1)\n",
    "y_valid_pred_ensemble = np.argmax(y_valid_pred_ensemble_proba, axis=1)\n",
    "y_test_pred_ensemble = np.argmax(y_test_pred_ensemble_proba, axis=1)\n",
    "\n",
    "# 计算各水质等级的得分\n",
    "print(\"=================CMAES=================\")\n",
    "\n",
    "print(classification_report(y_train, y_train_pred_ensemble, digits=4))\n",
    "print(classification_report(y_valid, y_valid_pred_ensemble, digits=4))\n",
    "print(classification_report(y_test, y_test_pred_ensemble, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人为设定了10个参数的初值x0=10*[0]，sigma0=0.5, 算法包中对sigma0的描述为：\n",
    "\n",
    "http://cma.gforge.inria.fr/apidocs-pycma/cma.evolution_strategy.CMAEvolutionStrategy.html\n",
    "\n",
    "> initial standard deviation. The problem variables should have been scaled, such that a single standard deviation on all variables is useful and the optimum is expected to lie within about x0 +- 3*sigma0. See also options scaling_of_variables. Often one wants to check for solutions close to the initial point. This allows, for example, for an easier check of consistency of the objective function and its interfacing with the optimizer. In this case, a much smaller sigma0 is advisable.\n",
    "\n",
    "CMAES 知乎算法专栏\n",
    "https://zhuanlan.zhihu.com/p/31193293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AdaptSigma': 'True  # or False or any CMAAdaptSigmaBase class e.g. CMAAdaptSigmaTPA, CMAAdaptSigmaCSA',\n",
       " 'BoundaryHandler': 'BoundTransform  # or BoundPenalty, unused when ``bounds in (None, [None, None])``',\n",
       " 'CMA_active': 'True  # negative update, conducted after the original update',\n",
       " 'CMA_cmean': '1  # learning rate for the mean value',\n",
       " 'CMA_const_trace': 'False  # normalize trace, 1, True, \"arithm\", \"geom\", \"aeig\", \"geig\" are valid',\n",
       " 'CMA_dampsvec_fac': 'np.Inf  # tentative and subject to changes, 0.5 would be a \"default\" damping for sigma vector update',\n",
       " 'CMA_dampsvec_fade': '0.1  # tentative fading out parameter for sigma vector update',\n",
       " 'CMA_diagonal': '0*100*N/popsize**0.5  # nb of iterations with diagonal covariance matrix, True for always',\n",
       " 'CMA_eigenmethod': 'np.linalg.eigh  # or cma.utils.eig or pygsl.eigen.eigenvectors',\n",
       " 'CMA_elitist': 'False  #v or \"initial\" or True, elitism likely impairs global search performance',\n",
       " 'CMA_mirrormethod': '2  # 0=unconditional, 1=selective, 2=selective with delay',\n",
       " 'CMA_mirrors': 'popsize < 6  # values <0.5 are interpreted as fraction, values >1 as numbers (rounded), otherwise about 0.16 is used',\n",
       " 'CMA_mu': 'None  # parents selection parameter, default is popsize // 2',\n",
       " 'CMA_on': '1  # multiplier for all covariance matrix updates',\n",
       " 'CMA_rankmu': '1.0  # multiplier for rank-mu update learning rate of covariance matrix',\n",
       " 'CMA_rankone': '1.0  # multiplier for rank-one update learning rate of covariance matrix',\n",
       " 'CMA_recombination_weights': 'None  # a list, see class RecombinationWeights, overwrites CMA_mu and popsize options',\n",
       " 'CMA_sampler': 'None  # a class or instance that implements the interface of `cma.interfaces.StatisticalModelSamplerWithZeroMeanBaseClass`',\n",
       " 'CMA_sampler_options': '{}  # options passed to `CMA_sampler` class init as keyword arguments',\n",
       " 'CMA_stds': 'None  # multipliers for sigma0 in each coordinate, not represented in C, makes scaling_of_variables obsolete',\n",
       " 'CMA_teststds': 'None  # factors for non-isotropic initial distr. of C, mainly for test purpose, see CMA_stds for production',\n",
       " 'CSA_clip_length_value': 'None  #v poorly tested, [0, 0] means const length N**0.5, [-1, 1] allows a variation of +- N/(N+2), etc.',\n",
       " 'CSA_damp_mueff_exponent': '0.5  # zero would mean no dependency of damping on mueff, useful with CSA_disregard_length option',\n",
       " 'CSA_dampfac': '1  #v positive multiplier for step-size damping, 0.3 is close to optimal on the sphere',\n",
       " 'CSA_disregard_length': 'False  #v True is untested, also changes respective parameters',\n",
       " 'CSA_squared': 'False  #v use squared length for sigma-adaptation ',\n",
       " 'bounds': '[None, None]  # lower (=bounds[0]) and upper domain boundaries, each a scalar or a list/vector',\n",
       " 'conditioncov_alleviate': '[1e8, 1e12]  # when to alleviate the condition in the coordinates and in main axes',\n",
       " 'fixed_variables': 'None  # dictionary with index-value pairs like {0:1.1, 2:0.1} that are not optimized',\n",
       " 'ftarget': '-inf  #v target function value, minimization',\n",
       " 'integer_variables': '[]  # index list, invokes basic integer handling: prevent std dev to become too small in the given variables',\n",
       " 'is_feasible': 'is_feasible  #v a function that computes feasibility, by default lambda x, f: f not in (None, np.NaN)',\n",
       " 'maxfevals': 'inf  #v maximum number of function evaluations',\n",
       " 'maxiter': '100 + 150 * (N+3)**2 // popsize**0.5  #v maximum number of iterations',\n",
       " 'maxstd': 'inf  #v maximal std in any coordinate direction',\n",
       " 'mean_shift_line_samples': 'False #v sample two new solutions colinear to previous mean shift',\n",
       " 'mindx': '0  #v minimal std in any arbitrary direction, cave interference with tol*',\n",
       " 'minstd': '0  #v minimal std (scalar or vector) in any coordinate direction, cave interference with tol*',\n",
       " 'pc_line_samples': 'False #v one line sample along the evolution path pc',\n",
       " 'popsize': '4+int(3*np.log(N))  # population size, AKA lambda, number of new solution per iteration',\n",
       " 'randn': 'np.random.randn  #v randn(lam, N) must return an np.array of shape (lam, N), see also cma.utilities.math.randhss',\n",
       " 'scaling_of_variables': 'None  # depreciated, rather use fitness_transformations.ScaleCoordinates instead (or possibly CMA_stds).\\n            Scale for each variable in that effective_sigma0 = sigma0*scaling. Internally the variables are divided by scaling_of_variables and sigma is unchanged, default is `np.ones(N)`',\n",
       " 'seed': 'time  # random number seed for `numpy.random`; `None` and `0` equate to `time`, `np.nan` means \"do nothing\", see also option \"randn\"',\n",
       " 'signals_filename': 'None  # cma_signals.in  # read versatile options from this file which contains a single options dict, e.g. ``{\"timeout\": 0}`` to stop, string-values are evaluated, e.g. \"np.inf\" is valid',\n",
       " 'termination_callback': 'None  #v a function returning True for termination, called in `stop` with `self` as argument, could be abused for side effects',\n",
       " 'timeout': 'inf  #v stop if timeout seconds are exceeded, the string \"2.5 * 60**2\" evaluates to 2 hours and 30 minutes',\n",
       " 'tolconditioncov': '1e14  #v stop if the condition of the covariance matrix is above `tolconditioncov`',\n",
       " 'tolfacupx': '1e3  #v termination when step-size increases by tolfacupx (diverges). That is, the initial step-size was chosen far too small and better solutions were found far away from the initial solution x0',\n",
       " 'tolfun': '1e-11  #v termination criterion: tolerance in function value, quite useful',\n",
       " 'tolfunhist': '1e-12  #v termination criterion: tolerance in function value history',\n",
       " 'tolstagnation': 'int(100 + 100 * N**1.5 / popsize)  #v termination if no improvement over tolstagnation iterations',\n",
       " 'tolupsigma': '1e20  #v sigma/sigma0 > tolupsigma * max(eivenvals(C)**0.5) indicates \"creeping behavior\" with usually minor improvements',\n",
       " 'tolx': '1e-11  #v termination criterion: tolerance in x-changes',\n",
       " 'transformation': 'None  # depreciated, use cma.fitness_transformations.FitnessTransformation instead.\\n            [t0, t1] are two mappings, t0 transforms solutions from CMA-representation to f-representation (tf_pheno),\\n            t1 is the (optional) back transformation, see class GenoPheno',\n",
       " 'typical_x': 'None  # used with scaling_of_variables',\n",
       " 'updatecovwait': 'None  #v number of iterations without distribution update, name is subject to future changes',\n",
       " 'verb_append': '0  # initial evaluation counter, if append, do not overwrite output files',\n",
       " 'verb_disp': '100  #v verbosity: display console output every verb_disp iteration',\n",
       " 'verb_filenameprefix': 'outcmaes  # output filenames prefix',\n",
       " 'verb_log': '1  #v verbosity: write data to files every verb_log iteration, writing can be time critical on fast to evaluate functions',\n",
       " 'verb_plot': '0  #v in fmin(): plot() is called every verb_plot iteration',\n",
       " 'verb_time': 'True  #v output timings on console',\n",
       " 'verbose': '3  #v verbosity e.g. of initial/final message, -1 is very quiet, -9 maximally quiet, may not be fully implemented',\n",
       " 'vv': '{}  #? versatile set or dictionary for hacking purposes, value found in self.opts[\"vv\"]'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 算法默认参数\n",
    "\n",
    "cma.CMAOptions.defaults()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
