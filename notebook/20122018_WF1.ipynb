{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda/envs/gc/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "from gcforest.gcforest import GCForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "kf=10\n",
    "score = 'f1_weighted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data_four_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33612, 5)\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()+'/../data/20122018freshwater_four_feature.csv'\n",
    "data_four_features = pd.read_csv(path, na_values = np.nan)\n",
    "\n",
    "print(data_four_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CE(1-9)标签y为1-6\n",
    "- CE(1-10)标签y为0-5\n",
    "\n",
    "先进行计算CE(1-9)，在载入CE(1-10)的时候要将标签统一减去1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data_four_features.drop(['本周水质'], axis=1) # Series\n",
    "y = data_four_features['本周水质'] # Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "水质分布情况:\n",
      "2    13272\n",
      "3     8797\n",
      "4     5472\n",
      "1     2438\n",
      "6     2146\n",
      "5     1487\n",
      "Name: 本周水质, dtype: int64\n",
      "\n",
      "各特征类型分布情况:\n",
      "float64    4\n",
      "int64      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = data_four_features.drop(['本周水质'], axis=1) # Series\n",
    "y = data_four_features['本周水质'] # Series\n",
    "print(\"水质分布情况:\")\n",
    "print(y.value_counts())\n",
    "print(\"\\n各特征类型分布情况:\")\n",
    "print(data_four_features.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert series to ndarray\n",
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ train_test_split ============\n",
      "67% train: 26889/33612, 33% test: 6723/33612\n"
     ]
    }
   ],
   "source": [
    "print(\"============ train_test_split ============\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                       stratify = y, random_state = random_seed)\n",
    "print(\"67%% train: %d/%d, 33%% test: %d/%d\" %(X_train.shape[0], X.shape[0], X_test.shape[0], X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize  train data\n",
    "\n",
    "fulfill the Na with median, then standardized the data, output type ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ train_valid_split ============\n"
     ]
    }
   ],
   "source": [
    "clean_pipeline = Pipeline([('imputer', preprocessing.Imputer(missing_values='NaN',strategy=\"median\")),\n",
    "                           ('std_scaler', preprocessing.StandardScaler()),])\n",
    "X_train = clean_pipeline.fit_transform(X_train)\n",
    "X_test = clean_pipeline.fit_transform(X_test)\n",
    "\n",
    "print(\"============ train_valid_split ============\")\n",
    "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, test_size=0.25, \n",
    "                                       stratify = y_train, random_state = random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X_train2: training set\n",
    "- X_valid: validation set\n",
    "- X_test: test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on 3 parts of data\n",
    "\n",
    "1. 载入CE（1-9）的子模型，仅计算CE（1-9），标签1-6\n",
    "2. 载入CE（1-10）的子模型覆盖之前的模型，计算10个子模型,标签0-5\n",
    "3. 计算CE（1-10）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 载入CE（1-9）的子模型，仅计算CE（1-9），标签1-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================CE(1-9)=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1     1.0000    0.9945    0.9973      1462\n",
      "          2     0.9987    0.9999    0.9993      7962\n",
      "          3     0.9991    0.9994    0.9992      5278\n",
      "          4     0.9994    0.9991    0.9992      3283\n",
      "          5     0.9989    0.9966    0.9978       893\n",
      "          6     0.9984    0.9984    0.9984      1288\n",
      "\n",
      "avg / total     0.9990    0.9990    0.9990     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1     0.9898    0.9918    0.9908       488\n",
      "          2     0.9959    0.9951    0.9955      2655\n",
      "          3     0.9938    0.9943    0.9940      1759\n",
      "          4     0.9945    0.9927    0.9936      1095\n",
      "          5     0.9833    0.9899    0.9866       297\n",
      "          6     0.9907    0.9907    0.9907       429\n",
      "\n",
      "avg / total     0.9938    0.9938    0.9938      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1     0.9099    0.9939    0.9500       488\n",
      "          2     0.9927    0.9763    0.9844      2655\n",
      "          3     0.9835    0.9801    0.9818      1760\n",
      "          4     0.9779    0.9707    0.9743      1094\n",
      "          5     0.9327    0.9327    0.9327       297\n",
      "          6     0.9593    0.9883    0.9736       429\n",
      "\n",
      "avg / total     0.9771    0.9765    0.9766      6723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    \"LogisticRegression\",\n",
    "    \"LinearDiscriminantAnalysis\",\n",
    "    \"SVC\",\n",
    "    \"DecisionTreeClassifier\",\n",
    "    \"ExtraTreeClassifier\",\n",
    "    \"GaussianNB\",\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"RandomForestClassifier\",\n",
    "    \"ExtraTreesClassifier\"\n",
    "]\n",
    "i=0\n",
    "for model in models:\n",
    "    model_name = model\n",
    "    with open(\"../pkl/CE_97661/CE_\" + model_name + \".pkl\", \"rb\") as f:\n",
    "        models[i] = pickle.load(f)\n",
    "    i = i+1\n",
    "# models 不再是字符数组，而是模型数组\n",
    "\n",
    "population_best_weight = np.load(\"../npy/CE_best_weights.npy\")\n",
    "\n",
    "classifier_num = 9\n",
    "\n",
    "# 所有学习器都输出概率向量，最后投票\n",
    "y_train_pred_proba_all = []\n",
    "y_valid_pred_proba_all = []\n",
    "y_test_pred_proba_all = []\n",
    "\n",
    "# 取训练好的模型，计算各模型”验证集“上输出概率向量\n",
    "for model in models:\n",
    "    train_pred_proba = model.predict_proba(X_train2)\n",
    "    valid_pred_proba = model.predict_proba(X_valid)\n",
    "    test_pred_proba = model.predict_proba(X_test)\n",
    "    y_train_pred_proba_all.append(train_pred_proba)\n",
    "    y_valid_pred_proba_all.append(valid_pred_proba)\n",
    "    y_test_pred_proba_all.append(test_pred_proba)\n",
    "    \n",
    "y_train_pred_ensemble_proba = np.zeros((len(y_train2), 6)) # 初始化集成器概率向量\n",
    "y_valid_pred_ensemble_proba = np.zeros((len(y_valid), 6)) # 初始化集成器概率向量\n",
    "y_test_pred_ensemble_proba = np.zeros((len(y_test), 6)) # 初始化集成器概率向量\n",
    "\n",
    "# 为每一个基学习器乘上权重\n",
    "for k in range(classifier_num):\n",
    "    y_train_pred_ensemble_proba += y_train_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_valid_pred_ensemble_proba += y_valid_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_test_pred_ensemble_proba += y_test_pred_proba_all[k] * population_best_weight[k]\n",
    "y_train_pred_ensemble = np.argmax(y_train_pred_ensemble_proba, axis=1) + 1\n",
    "y_valid_pred_ensemble = np.argmax(y_valid_pred_ensemble_proba, axis=1) + 1\n",
    "y_test_pred_ensemble = np.argmax(y_test_pred_ensemble_proba, axis=1) + 1\n",
    "\n",
    "# 计算各水质等级的得分\n",
    "print(\"=================CE(1-9)=================\")\n",
    "\n",
    "print(classification_report(y_train2, y_train_pred_ensemble, digits=4))\n",
    "print(classification_report(y_valid, y_valid_pred_ensemble, digits=4))\n",
    "print(classification_report(y_test, y_test_pred_ensemble, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 载入CE（1-10）的子模型覆盖之前的模型，计算10个子模型,标签0-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train2 = y_train2-1\n",
    "y_valid = y_valid-1\n",
    "y_test = y_test-1\n",
    "\n",
    "models = [\n",
    "    \"LogisticRegression\",\n",
    "    \"LinearDiscriminantAnalysis\",\n",
    "    \"SVC\",\n",
    "    \"DecisionTreeClassifier\",\n",
    "    \"ExtraTreeClassifier\",\n",
    "    \"GaussianNB\",\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"RandomForestClassifier\",\n",
    "    \"ExtraTreesClassifier\",\n",
    "    \"GCForest\"\n",
    "]\n",
    "\n",
    "i=0\n",
    "for name in models:\n",
    "    model_path = \"../pkl/CE_97661_10/CE_\" + name + \".pkl\"\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        models[i] = pickle.load(f)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================LogisticRegression=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8654    0.0308    0.0594      1462\n",
      "          1     0.6703    0.9921    0.8000      7962\n",
      "          2     0.6432    0.5722    0.6056      5278\n",
      "          3     0.6217    0.4505    0.5224      3283\n",
      "          4     0.0000    0.0000    0.0000       893\n",
      "          5     0.7904    0.7702    0.7802      1288\n",
      "\n",
      "avg / total     0.6474    0.6662    0.6136     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9375    0.0307    0.0595       488\n",
      "          1     0.6764    0.9913    0.8042      2655\n",
      "          2     0.6439    0.5850    0.6130      1759\n",
      "          3     0.6338    0.4521    0.5277      1095\n",
      "          4     0.0000    0.0000    0.0000       297\n",
      "          5     0.7735    0.7879    0.7806       429\n",
      "\n",
      "avg / total     0.6562    0.6707    0.6181      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9333    0.0574    0.1081       488\n",
      "          1     0.6672    0.9951    0.7988      2655\n",
      "          2     0.6628    0.5517    0.6022      1760\n",
      "          3     0.6360    0.4808    0.5476      1094\n",
      "          4     0.0000    0.0000    0.0000       297\n",
      "          5     0.7818    0.8019    0.7917       429\n",
      "\n",
      "avg / total     0.6581    0.6710    0.6206      6723\n",
      "\n",
      "=================LinearDiscriminantAnalysis=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.0000    0.0000    0.0000      1462\n",
      "          1     0.5995    0.9990    0.7494      7962\n",
      "          2     0.5908    0.3808    0.4631      5278\n",
      "          3     0.6896    0.5062    0.5839      3283\n",
      "          4     0.5030    0.2833    0.3625       893\n",
      "          5     0.9863    0.4457    0.6139      1288\n",
      "\n",
      "avg / total     0.5889    0.6175    0.5674     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.0000    0.0000    0.0000       488\n",
      "          1     0.6000    0.9985    0.7496      2655\n",
      "          2     0.5918    0.3849    0.4664      1759\n",
      "          3     0.7011    0.5142    0.5933      1095\n",
      "          4     0.5031    0.2694    0.3509       297\n",
      "          5     0.9848    0.4545    0.6220       429\n",
      "\n",
      "avg / total     0.5911    0.6197    0.5699      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.0000    0.0000    0.0000       488\n",
      "          1     0.6016    0.9992    0.7510      2655\n",
      "          2     0.6036    0.3858    0.4707      1760\n",
      "          3     0.7075    0.5283    0.6049      1094\n",
      "          4     0.5361    0.2997    0.3844       297\n",
      "          5     0.9902    0.4732    0.6404       429\n",
      "\n",
      "avg / total     0.5976    0.6250    0.5761      6723\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda/envs/gc/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================SVC=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8607    0.6464    0.7383      1462\n",
      "          1     0.8850    0.9362    0.9099      7962\n",
      "          2     0.8812    0.8920    0.8865      5278\n",
      "          3     0.9375    0.9007    0.9188      3283\n",
      "          4     0.8914    0.8735    0.8824       893\n",
      "          5     0.9686    0.9573    0.9629      1288\n",
      "\n",
      "avg / total     0.8964    0.8964    0.8949     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8509    0.6434    0.7328       488\n",
      "          1     0.8802    0.9322    0.9054      2655\n",
      "          2     0.8760    0.8874    0.8817      1759\n",
      "          3     0.9239    0.8977    0.9106      1095\n",
      "          4     0.8826    0.7845    0.8307       297\n",
      "          5     0.9421    0.9487    0.9454       429\n",
      "\n",
      "avg / total     0.8881    0.8884    0.8868      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7970    0.6516    0.7170       488\n",
      "          1     0.8790    0.9333    0.9054      2655\n",
      "          2     0.9012    0.8761    0.8885      1760\n",
      "          3     0.9366    0.9177    0.9271      1094\n",
      "          4     0.9253    0.8754    0.8997       297\n",
      "          5     0.9546    0.9814    0.9678       429\n",
      "\n",
      "avg / total     0.8951    0.8959    0.8945      6723\n",
      "\n",
      "=================DecisionTreeClassifier=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    1.0000    1.0000      1462\n",
      "          1     1.0000    1.0000    1.0000      7962\n",
      "          2     1.0000    1.0000    1.0000      5278\n",
      "          3     1.0000    1.0000    1.0000      3283\n",
      "          4     1.0000    1.0000    1.0000       893\n",
      "          5     1.0000    1.0000    1.0000      1288\n",
      "\n",
      "avg / total     1.0000    1.0000    1.0000     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9758    0.9898    0.9827       488\n",
      "          1     0.9913    0.9891    0.9902      2655\n",
      "          2     0.9858    0.9875    0.9867      1759\n",
      "          3     0.9926    0.9826    0.9876      1095\n",
      "          4     0.9669    0.9832    0.9750       297\n",
      "          5     0.9861    0.9907    0.9884       429\n",
      "\n",
      "avg / total     0.9875    0.9875    0.9875      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8994    0.9898    0.9424       488\n",
      "          1     0.9874    0.9718    0.9795      2655\n",
      "          2     0.9783    0.9722    0.9752      1760\n",
      "          3     0.9741    0.9644    0.9692      1094\n",
      "          4     0.9186    0.9125    0.9155       297\n",
      "          5     0.9507    0.9883    0.9691       429\n",
      "\n",
      "avg / total     0.9711    0.9704    0.9705      6723\n",
      "\n",
      "=================ExtraTreeClassifier=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    1.0000    1.0000      1462\n",
      "          1     1.0000    1.0000    1.0000      7962\n",
      "          2     1.0000    1.0000    1.0000      5278\n",
      "          3     1.0000    1.0000    1.0000      3283\n",
      "          4     1.0000    1.0000    1.0000       893\n",
      "          5     1.0000    1.0000    1.0000      1288\n",
      "\n",
      "avg / total     1.0000    1.0000    1.0000     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8774    0.9242    0.9002       488\n",
      "          1     0.9471    0.9303    0.9386      2655\n",
      "          2     0.8741    0.8999    0.8868      1759\n",
      "          3     0.8686    0.8511    0.8598      1095\n",
      "          4     0.6935    0.7239    0.7084       297\n",
      "          5     0.9091    0.8625    0.8852       429\n",
      "\n",
      "avg / total     0.8965    0.8956    0.8959      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8346    0.8996    0.8659       488\n",
      "          1     0.9418    0.9330    0.9374      2655\n",
      "          2     0.8898    0.8989    0.8943      1760\n",
      "          3     0.8643    0.8382    0.8510      1094\n",
      "          4     0.7222    0.7441    0.7330       297\n",
      "          5     0.9076    0.8928    0.9001       429\n",
      "\n",
      "avg / total     0.8959    0.8953    0.8955      6723\n",
      "\n",
      "=================GaussianNB=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8407    0.7832    0.8109      1462\n",
      "          1     0.8573    0.9094    0.8826      7962\n",
      "          2     0.7624    0.8230    0.7915      5278\n",
      "          3     0.7706    0.6957    0.7312      3283\n",
      "          4     0.6671    0.6013    0.6325       893\n",
      "          5     0.9787    0.6770    0.8004      1288\n",
      "\n",
      "avg / total     0.8165    0.8144    0.8126     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8256    0.8053    0.8154       488\n",
      "          1     0.8606    0.9047    0.8821      2655\n",
      "          2     0.7600    0.8192    0.7885      1759\n",
      "          3     0.7755    0.7005    0.7361      1095\n",
      "          4     0.6448    0.5623    0.6007       297\n",
      "          5     0.9808    0.7133    0.8259       429\n",
      "\n",
      "avg / total     0.8160    0.8145    0.8130      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7886    0.7951    0.7918       488\n",
      "          1     0.8657    0.9081    0.8864      2655\n",
      "          2     0.7909    0.8403    0.8149      1760\n",
      "          3     0.7816    0.7130    0.7457      1094\n",
      "          4     0.6560    0.5522    0.5996       297\n",
      "          5     0.9695    0.7413    0.8402       429\n",
      "\n",
      "avg / total     0.8242    0.8240    0.8223      6723\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-02-11 17:04:46,419][cascade_classifier.transform] X_groups_test.shape=[(20166, 4)]\n",
      "[ 2019-02-11 17:04:46,420][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 17:04:46,421][cascade_classifier.transform] X_test.shape=(20166, 4)\n",
      "[ 2019-02-11 17:04:46,422][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(20166, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================KNeighborsClassifier=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8741    0.8406    0.8570      1462\n",
      "          1     0.9355    0.9651    0.9500      7962\n",
      "          2     0.9457    0.9343    0.9400      5278\n",
      "          3     0.9520    0.9421    0.9470      3283\n",
      "          4     0.9208    0.8589    0.8888       893\n",
      "          5     0.9776    0.9488    0.9630      1288\n",
      "\n",
      "avg / total     0.9384    0.9385    0.9383     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7908    0.7746    0.7826       488\n",
      "          1     0.8996    0.9348    0.9169      2655\n",
      "          2     0.9013    0.8931    0.8972      1759\n",
      "          3     0.9092    0.8868    0.8978      1095\n",
      "          4     0.7879    0.7003    0.7415       297\n",
      "          5     0.9440    0.9044    0.9238       429\n",
      "\n",
      "avg / total     0.8916    0.8922    0.8916      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7530    0.7684    0.7606       488\n",
      "          1     0.9016    0.9315    0.9163      2655\n",
      "          2     0.9135    0.8875    0.9003      1760\n",
      "          3     0.9245    0.8958    0.9099      1094\n",
      "          4     0.8467    0.8182    0.8322       297\n",
      "          5     0.9482    0.9394    0.9438       429\n",
      "\n",
      "avg / total     0.8982    0.8978    0.8978      6723\n",
      "\n",
      "=================RandomForestClassifier=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9993    0.9966    0.9979      1462\n",
      "          1     0.9990    0.9997    0.9994      7962\n",
      "          2     0.9989    0.9994    0.9991      5278\n",
      "          3     0.9997    0.9985    0.9991      3283\n",
      "          4     0.9989    0.9966    0.9978       893\n",
      "          5     0.9984    0.9992    0.9988      1288\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9991     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9898    0.9918    0.9908       488\n",
      "          1     0.9966    0.9951    0.9959      2655\n",
      "          2     0.9938    0.9955    0.9946      1759\n",
      "          3     0.9945    0.9927    0.9936      1095\n",
      "          4     0.9865    0.9865    0.9865       297\n",
      "          5     0.9884    0.9930    0.9907       429\n",
      "\n",
      "avg / total     0.9941    0.9941    0.9941      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9065    0.9939    0.9482       488\n",
      "          1     0.9927    0.9755    0.9840      2655\n",
      "          2     0.9835    0.9801    0.9818      1760\n",
      "          3     0.9779    0.9698    0.9738      1094\n",
      "          4     0.9295    0.9327    0.9311       297\n",
      "          5     0.9593    0.9883    0.9736       429\n",
      "\n",
      "avg / total     0.9767    0.9761    0.9762      6723\n",
      "\n",
      "=================ExtraTreesClassifier=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    1.0000    1.0000      1462\n",
      "          1     1.0000    1.0000    1.0000      7962\n",
      "          2     1.0000    1.0000    1.0000      5278\n",
      "          3     1.0000    1.0000    1.0000      3283\n",
      "          4     1.0000    1.0000    1.0000       893\n",
      "          5     1.0000    1.0000    1.0000      1288\n",
      "\n",
      "avg / total     1.0000    1.0000    1.0000     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9619    0.9836    0.9726       488\n",
      "          1     0.9835    0.9864    0.9850      2655\n",
      "          2     0.9717    0.9756    0.9736      1759\n",
      "          3     0.9697    0.9635    0.9666      1095\n",
      "          4     0.9321    0.8788    0.9047       297\n",
      "          5     0.9672    0.9627    0.9650       429\n",
      "\n",
      "avg / total     0.9733    0.9734    0.9733      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8905    0.9836    0.9348       488\n",
      "          1     0.9803    0.9721    0.9762      2655\n",
      "          2     0.9710    0.9528    0.9619      1760\n",
      "          3     0.9506    0.9497    0.9502      1094\n",
      "          4     0.9189    0.9158    0.9174       297\n",
      "          5     0.9701    0.9837    0.9769       429\n",
      "\n",
      "avg / total     0.9631    0.9625    0.9626      6723\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-02-11 17:04:46,869][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 17:04:47,333][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 17:04:47,781][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 17:04:48,235][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 17:04:48,692][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 17:04:49,178][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-11 17:04:49,179][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 17:04:49,180][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-11 17:04:49,181][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-11 17:04:49,378][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:04:49,583][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:04:49,780][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:04:49,952][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:04:50,117][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:04:50,322][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-11 17:04:50,323][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 17:04:50,324][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-11 17:04:50,325][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-11 17:04:50,478][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:04:50,691][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:04:50,870][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:04:51,112][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:04:51,293][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(6723, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================GCForest=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9979    0.9966    0.9973      1462\n",
      "          1     0.9992    0.9995    0.9994      7962\n",
      "          2     1.0000    0.9983    0.9991      5278\n",
      "          3     0.9985    0.9997    0.9991      3283\n",
      "          4     0.9944    1.0000    0.9972       893\n",
      "          5     1.0000    1.0000    1.0000      1288\n",
      "\n",
      "avg / total     0.9991    0.9991    0.9991     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9898    0.9939    0.9918       488\n",
      "          1     0.9970    0.9951    0.9960      2655\n",
      "          2     0.9938    0.9949    0.9943      1759\n",
      "          3     0.9927    0.9927    0.9927      1095\n",
      "          4     0.9833    0.9899    0.9866       297\n",
      "          5     0.9930    0.9907    0.9918       429\n",
      "\n",
      "avg / total     0.9941    0.9941    0.9941      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9082    0.9939    0.9491       488\n",
      "          1     0.9927    0.9759    0.9842      2655\n",
      "          2     0.9835    0.9801    0.9818      1760\n",
      "          3     0.9779    0.9698    0.9738      1094\n",
      "          4     0.9327    0.9327    0.9327       297\n",
      "          5     0.9594    0.9907    0.9748       429\n",
      "\n",
      "avg / total     0.9770    0.9763    0.9765      6723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    train_pred = model.predict(X_train2)\n",
    "    valid_pred = model.predict(X_valid)\n",
    "    test_pred = model.predict(X_test)\n",
    "    print(\"=================\" + model_name + \"=================\")\n",
    "    print(classification_report(y_train2, train_pred, digits=4))\n",
    "    print(classification_report(y_valid, valid_pred, digits=4))\n",
    "    print(classification_report(y_test, test_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 计算CE（1-10）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-02-11 17:04:57,190][cascade_classifier.transform] X_groups_test.shape=[(20166, 4)]\n",
      "[ 2019-02-11 17:04:57,194][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 17:04:57,195][cascade_classifier.transform] X_test.shape=(20166, 4)\n",
      "[ 2019-02-11 17:04:57,196][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(20166, 4)\n",
      "[ 2019-02-11 17:04:57,627][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 17:04:58,090][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 17:04:58,536][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 17:04:58,978][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 17:04:59,425][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 17:04:59,880][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-11 17:04:59,881][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 17:04:59,881][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-11 17:04:59,882][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-11 17:05:00,051][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:05:00,235][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:05:00,406][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:05:00,571][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:05:00,747][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:05:00,923][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-11 17:05:00,924][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 17:05:00,925][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-11 17:05:00,926][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-11 17:05:01,089][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:05:01,273][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:05:01,443][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:05:01,608][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 17:05:01,790][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(6723, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================CE(1-10)=================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9979    0.9938    0.9959      1462\n",
      "          1     0.9985    0.9986    0.9986      7962\n",
      "          2     0.9989    0.9985    0.9987      5278\n",
      "          3     0.9979    0.9991    0.9985      3283\n",
      "          4     0.9978    1.0000    0.9989       893\n",
      "          5     0.9992    1.0000    0.9996      1288\n",
      "\n",
      "avg / total     0.9985    0.9985    0.9985     20166\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9898    0.9939    0.9918       488\n",
      "          1     0.9970    0.9951    0.9960      2655\n",
      "          2     0.9938    0.9949    0.9943      1759\n",
      "          3     0.9936    0.9927    0.9931      1095\n",
      "          4     0.9800    0.9899    0.9849       297\n",
      "          5     0.9930    0.9907    0.9918       429\n",
      "\n",
      "avg / total     0.9941    0.9941    0.9941      6723\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9099    0.9939    0.9500       488\n",
      "          1     0.9927    0.9763    0.9844      2655\n",
      "          2     0.9835    0.9801    0.9818      1760\n",
      "          3     0.9779    0.9698    0.9738      1094\n",
      "          4     0.9327    0.9327    0.9327       297\n",
      "          5     0.9594    0.9907    0.9748       429\n",
      "\n",
      "avg / total     0.9771    0.9765    0.9766      6723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "population_best_weight = np.load(\"../npy/CE_best_weights(1-10).npy\")\n",
    "\n",
    "classifier_num = 10\n",
    "\n",
    "# 所有学习器都输出概率向量，最后投票\n",
    "y_train_pred_proba_all = []\n",
    "y_valid_pred_proba_all = []\n",
    "y_test_pred_proba_all = []\n",
    "\n",
    "# 取训练好的模型，计算各模型”验证集“上输出概率向量\n",
    "for model in models:\n",
    "    train_pred_proba = model.predict_proba(X_train2)\n",
    "    valid_pred_proba = model.predict_proba(X_valid)\n",
    "    test_pred_proba = model.predict_proba(X_test)\n",
    "    y_train_pred_proba_all.append(train_pred_proba)\n",
    "    y_valid_pred_proba_all.append(valid_pred_proba)\n",
    "    y_test_pred_proba_all.append(test_pred_proba)\n",
    "    \n",
    "y_train_pred_ensemble_proba = np.zeros((len(y_train2), 6)) # 初始化集成器概率向量\n",
    "y_valid_pred_ensemble_proba = np.zeros((len(y_valid), 6)) # 初始化集成器概率向量\n",
    "y_test_pred_ensemble_proba = np.zeros((len(y_test), 6)) # 初始化集成器概率向量\n",
    "\n",
    "# 为每一个基学习器乘上权重\n",
    "for k in range(classifier_num):\n",
    "    y_train_pred_ensemble_proba += y_train_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_valid_pred_ensemble_proba += y_valid_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_test_pred_ensemble_proba += y_test_pred_proba_all[k] * population_best_weight[k]\n",
    "y_train_pred_ensemble = np.argmax(y_train_pred_ensemble_proba, axis=1)\n",
    "y_valid_pred_ensemble = np.argmax(y_valid_pred_ensemble_proba, axis=1)\n",
    "y_test_pred_ensemble = np.argmax(y_test_pred_ensemble_proba, axis=1)\n",
    "\n",
    "# 计算各水质等级的得分\n",
    "print(\"=================CE(1-10)=================\")\n",
    "\n",
    "print(classification_report(y_train2, y_train_pred_ensemble, digits=4))\n",
    "print(classification_report(y_valid, y_valid_pred_ensemble, digits=4))\n",
    "print(classification_report(y_test, y_test_pred_ensemble, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gc]",
   "language": "python",
   "name": "conda-env-gc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
