{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda/envs/gc/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "from gcforest.gcforest import GCForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "kf=10\n",
    "score = 'f1_weighted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data_four_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33612, 5)\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()+'/../data/20122018freshwater_four_feature.csv'\n",
    "data_four_features = pd.read_csv(path, na_values = np.nan)\n",
    "\n",
    "print(data_four_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CE(1-9)标签y为1-6\n",
    "- CE(1-10)标签y为0-5\n",
    "\n",
    "先进行计算CE(1-9)，在载入CE(1-10)的时候要将标签统一减去1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data_four_features.drop(['本周水质'], axis=1) # Series\n",
    "y = data_four_features['本周水质'] # Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "水质分布情况:\n",
      "2    13272\n",
      "3     8797\n",
      "4     5472\n",
      "1     2438\n",
      "6     2146\n",
      "5     1487\n",
      "Name: 本周水质, dtype: int64\n",
      "\n",
      "各特征类型分布情况:\n",
      "float64    4\n",
      "int64      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = data_four_features.drop(['本周水质'], axis=1) # Series\n",
    "y = data_four_features['本周水质'] # Series\n",
    "print(\"水质分布情况:\")\n",
    "print(y.value_counts())\n",
    "print(\"\\n各特征类型分布情况:\")\n",
    "print(data_four_features.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert series to ndarray\n",
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ train_test_split ============\n",
      "67% train: 26889/33612, 33% test: 6723/33612\n"
     ]
    }
   ],
   "source": [
    "print(\"============ train_test_split ============\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                       stratify = y, random_state = random_seed)\n",
    "print(\"67%% train: %d/%d, 33%% test: %d/%d\" %(X_train.shape[0], X.shape[0], X_test.shape[0], X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize  train data\n",
    "\n",
    "fulfill the Na with median, then standardized the data, output type ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ train_valid_split ============\n"
     ]
    }
   ],
   "source": [
    "clean_pipeline = Pipeline([('imputer', preprocessing.Imputer(missing_values='NaN',strategy=\"median\")),\n",
    "                           ('std_scaler', preprocessing.StandardScaler()),])\n",
    "X_train = clean_pipeline.fit_transform(X_train)\n",
    "X_test = clean_pipeline.fit_transform(X_test)\n",
    "\n",
    "print(\"============ train_valid_split ============\")\n",
    "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, test_size=0.25, \n",
    "                                       stratify = y_train, random_state = random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X_train2: training set\n",
    "- X_valid: validation set\n",
    "- X_test: test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on 3 parts of data\n",
    "\n",
    "1. 载入CE（1-9）的子模型，仅计算CE（1-9），标签1-6\n",
    "2. 载入CE（1-10）的子模型覆盖之前的模型，计算10个子模型,标签0-5\n",
    "3. 计算CE（1-10）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 载入CE（1-9）的子模型，仅计算CE（1-9），标签1-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================CE(1-9)=================\n",
      "1 train_acc: 99.45\n",
      "2 train_acc: 99.99\n",
      "3 train_acc: 99.94\n",
      "4 train_acc: 99.91\n",
      "5 train_acc: 99.66\n",
      "6 train_acc: 99.84\n",
      "average: 99.80\n",
      "1 valid_acc: 99.18\n",
      "2 valid_acc: 99.51\n",
      "3 valid_acc: 99.43\n",
      "4 valid_acc: 99.27\n",
      "5 valid_acc: 98.99\n",
      "6 valid_acc: 99.07\n",
      "average: 99.24\n",
      "1 test_acc: 99.39\n",
      "2 test_acc: 97.63\n",
      "3 test_acc: 98.01\n",
      "4 test_acc: 97.07\n",
      "5 test_acc: 93.27\n",
      "6 test_acc: 98.83\n",
      "average: 97.37\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    \"LogisticRegression\",\n",
    "    \"LinearDiscriminantAnalysis\",\n",
    "    \"SVC\",\n",
    "    \"DecisionTreeClassifier\",\n",
    "    \"ExtraTreeClassifier\",\n",
    "    \"GaussianNB\",\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"RandomForestClassifier\",\n",
    "    \"ExtraTreesClassifier\"\n",
    "]\n",
    "i=0\n",
    "for model in models:\n",
    "    model_name = model\n",
    "    with open(\"../pkl/CE_97661/CE_\" + model_name + \".pkl\", \"rb\") as f:\n",
    "        models[i] = pickle.load(f)\n",
    "    i = i+1\n",
    "# models 不再是字符数组，而是模型数组\n",
    "\n",
    "population_best_weight = np.load(\"../npy/CE_best_weights.npy\")\n",
    "\n",
    "classifier_num = 9\n",
    "\n",
    "# 所有学习器都输出概率向量，最后投票\n",
    "y_train_pred_proba_all = []\n",
    "y_valid_pred_proba_all = []\n",
    "y_test_pred_proba_all = []\n",
    "\n",
    "# 取训练好的模型，计算各模型”验证集“上输出概率向量\n",
    "for model in models:\n",
    "    train_pred_proba = model.predict_proba(X_train2)\n",
    "    valid_pred_proba = model.predict_proba(X_valid)\n",
    "    test_pred_proba = model.predict_proba(X_test)\n",
    "    y_train_pred_proba_all.append(train_pred_proba)\n",
    "    y_valid_pred_proba_all.append(valid_pred_proba)\n",
    "    y_test_pred_proba_all.append(test_pred_proba)\n",
    "    \n",
    "y_train_pred_ensemble_proba = np.zeros((len(y_train2), 6)) # 初始化集成器概率向量\n",
    "y_valid_pred_ensemble_proba = np.zeros((len(y_valid), 6)) # 初始化集成器概率向量\n",
    "y_test_pred_ensemble_proba = np.zeros((len(y_test), 6)) # 初始化集成器概率向量\n",
    "\n",
    "# 为每一个基学习器乘上权重\n",
    "for k in range(classifier_num):\n",
    "    y_train_pred_ensemble_proba += y_train_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_valid_pred_ensemble_proba += y_valid_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_test_pred_ensemble_proba += y_test_pred_proba_all[k] * population_best_weight[k]\n",
    "y_train_pred_ensemble = np.argmax(y_train_pred_ensemble_proba, axis=1) + 1\n",
    "y_valid_pred_ensemble = np.argmax(y_valid_pred_ensemble_proba, axis=1) + 1\n",
    "y_test_pred_ensemble = np.argmax(y_test_pred_ensemble_proba, axis=1) + 1\n",
    "\n",
    "# 计算各水质等级的得分\n",
    "print(\"=================CE(1-9)=================\")\n",
    "train_cm = confusion_matrix(y_train2, y_train_pred_ensemble)\n",
    "valid_cm = confusion_matrix(y_valid, y_valid_pred_ensemble)\n",
    "test_cm = confusion_matrix(y_test, y_test_pred_ensemble)\n",
    "i=0\n",
    "train_acc_all = np.zeros(6)\n",
    "for c in train_cm:\n",
    "    train_acc_all[i] = c[i]/np.sum(c)\n",
    "    print(\"%d train_acc: %.2f\" %(i+1, 100*train_acc_all[i]))\n",
    "    i=i+1\n",
    "print(\"average: %.2f\" % (100*np.mean(train_acc_all)))\n",
    "i=0\n",
    "valid_acc_all = np.zeros(6)\n",
    "for c in valid_cm:\n",
    "    valid_acc_all[i] = c[i]/np.sum(c)\n",
    "    print(\"%d valid_acc: %.2f\" %(i+1, 100*valid_acc_all[i]))\n",
    "    i=i+1\n",
    "print(\"average: %.2f\" % (100*np.mean(valid_acc_all)))\n",
    "i=0\n",
    "test_acc_all = np.zeros(6)\n",
    "for c in test_cm:\n",
    "    test_acc_all[i] = c[i]/np.sum(c)\n",
    "    print(\"%d test_acc: %.2f\" %(i+1, 100*test_acc_all[i]))\n",
    "    i=i+1\n",
    "print(\"average: %.2f\" % (100*np.mean(test_acc_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 载入CE（1-10）的子模型覆盖之前的模型，计算10个子模型,标签0-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train2 = y_train2-1\n",
    "y_valid = y_valid-1\n",
    "y_test = y_test-1\n",
    "\n",
    "models = [\n",
    "    \"LogisticRegression\",\n",
    "    \"LinearDiscriminantAnalysis\",\n",
    "    \"SVC\",\n",
    "    \"DecisionTreeClassifier\",\n",
    "    \"ExtraTreeClassifier\",\n",
    "    \"GaussianNB\",\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"RandomForestClassifier\",\n",
    "    \"ExtraTreesClassifier\",\n",
    "    \"GCForest\"\n",
    "]\n",
    "\n",
    "i=0\n",
    "for name in models:\n",
    "    model_path = \"../pkl/CE_97661_10/CE_\" + name + \".pkl\"\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        models[i] = pickle.load(f)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================LogisticRegression=================\n",
      "1 train_acc: 3.08\n",
      "2 train_acc: 99.21\n",
      "3 train_acc: 57.22\n",
      "4 train_acc: 45.05\n",
      "5 train_acc: 0.00\n",
      "6 train_acc: 77.02\n",
      "average: 46.93\n",
      "1 valid_acc: 3.07\n",
      "2 valid_acc: 99.13\n",
      "3 valid_acc: 58.50\n",
      "4 valid_acc: 45.21\n",
      "5 valid_acc: 0.00\n",
      "6 valid_acc: 78.79\n",
      "average: 47.45\n",
      "1 test_acc: 5.74\n",
      "2 test_acc: 99.51\n",
      "3 test_acc: 55.17\n",
      "4 test_acc: 48.08\n",
      "5 test_acc: 0.00\n",
      "6 test_acc: 80.19\n",
      "average: 48.11\n",
      "=================LinearDiscriminantAnalysis=================\n",
      "1 train_acc: 0.00\n",
      "2 train_acc: 99.90\n",
      "3 train_acc: 38.08\n",
      "4 train_acc: 50.62\n",
      "5 train_acc: 28.33\n",
      "6 train_acc: 44.57\n",
      "average: 43.58\n",
      "1 valid_acc: 0.00\n",
      "2 valid_acc: 99.85\n",
      "3 valid_acc: 38.49\n",
      "4 valid_acc: 51.42\n",
      "5 valid_acc: 26.94\n",
      "6 valid_acc: 45.45\n",
      "average: 43.69\n",
      "1 test_acc: 0.00\n",
      "2 test_acc: 99.92\n",
      "3 test_acc: 38.58\n",
      "4 test_acc: 52.83\n",
      "5 test_acc: 29.97\n",
      "6 test_acc: 47.32\n",
      "average: 44.77\n",
      "=================SVC=================\n",
      "1 train_acc: 64.64\n",
      "2 train_acc: 93.62\n",
      "3 train_acc: 89.20\n",
      "4 train_acc: 90.07\n",
      "5 train_acc: 87.35\n",
      "6 train_acc: 95.73\n",
      "average: 86.77\n",
      "1 valid_acc: 64.34\n",
      "2 valid_acc: 93.22\n",
      "3 valid_acc: 88.74\n",
      "4 valid_acc: 89.77\n",
      "5 valid_acc: 78.45\n",
      "6 valid_acc: 94.87\n",
      "average: 84.90\n",
      "1 test_acc: 65.16\n",
      "2 test_acc: 93.33\n",
      "3 test_acc: 87.61\n",
      "4 test_acc: 91.77\n",
      "5 test_acc: 87.54\n",
      "6 test_acc: 98.14\n",
      "average: 87.26\n",
      "=================DecisionTreeClassifier=================\n",
      "1 train_acc: 100.00\n",
      "2 train_acc: 100.00\n",
      "3 train_acc: 100.00\n",
      "4 train_acc: 100.00\n",
      "5 train_acc: 100.00\n",
      "6 train_acc: 100.00\n",
      "average: 100.00\n",
      "1 valid_acc: 98.98\n",
      "2 valid_acc: 98.91\n",
      "3 valid_acc: 98.75\n",
      "4 valid_acc: 98.26\n",
      "5 valid_acc: 98.32\n",
      "6 valid_acc: 99.07\n",
      "average: 98.71\n",
      "1 test_acc: 98.98\n",
      "2 test_acc: 97.18\n",
      "3 test_acc: 97.22\n",
      "4 test_acc: 96.44\n",
      "5 test_acc: 91.25\n",
      "6 test_acc: 98.83\n",
      "average: 96.65\n",
      "=================ExtraTreeClassifier=================\n",
      "1 train_acc: 100.00\n",
      "2 train_acc: 100.00\n",
      "3 train_acc: 100.00\n",
      "4 train_acc: 100.00\n",
      "5 train_acc: 100.00\n",
      "6 train_acc: 100.00\n",
      "average: 100.00\n",
      "1 valid_acc: 92.42\n",
      "2 valid_acc: 93.03\n",
      "3 valid_acc: 89.99\n",
      "4 valid_acc: 85.11\n",
      "5 valid_acc: 72.39\n",
      "6 valid_acc: 86.25\n",
      "average: 86.53\n",
      "1 test_acc: 89.96\n",
      "2 test_acc: 93.30\n",
      "3 test_acc: 89.89\n",
      "4 test_acc: 83.82\n",
      "5 test_acc: 74.41\n",
      "6 test_acc: 89.28\n",
      "average: 86.78\n",
      "=================GaussianNB=================\n",
      "1 train_acc: 78.32\n",
      "2 train_acc: 90.94\n",
      "3 train_acc: 82.30\n",
      "4 train_acc: 69.57\n",
      "5 train_acc: 60.13\n",
      "6 train_acc: 67.70\n",
      "average: 74.83\n",
      "1 valid_acc: 80.53\n",
      "2 valid_acc: 90.47\n",
      "3 valid_acc: 81.92\n",
      "4 valid_acc: 70.05\n",
      "5 valid_acc: 56.23\n",
      "6 valid_acc: 71.33\n",
      "average: 75.09\n",
      "1 test_acc: 79.51\n",
      "2 test_acc: 90.81\n",
      "3 test_acc: 84.03\n",
      "4 test_acc: 71.30\n",
      "5 test_acc: 55.22\n",
      "6 test_acc: 74.13\n",
      "average: 75.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-02-11 16:31:14,501][cascade_classifier.transform] X_groups_test.shape=[(20166, 4)]\n",
      "[ 2019-02-11 16:31:14,503][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 16:31:14,504][cascade_classifier.transform] X_test.shape=(20166, 4)\n",
      "[ 2019-02-11 16:31:14,505][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(20166, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================KNeighborsClassifier=================\n",
      "1 train_acc: 84.06\n",
      "2 train_acc: 96.51\n",
      "3 train_acc: 93.43\n",
      "4 train_acc: 94.21\n",
      "5 train_acc: 85.89\n",
      "6 train_acc: 94.88\n",
      "average: 91.50\n",
      "1 valid_acc: 77.46\n",
      "2 valid_acc: 93.48\n",
      "3 valid_acc: 89.31\n",
      "4 valid_acc: 88.68\n",
      "5 valid_acc: 70.03\n",
      "6 valid_acc: 90.44\n",
      "average: 84.90\n",
      "1 test_acc: 76.84\n",
      "2 test_acc: 93.15\n",
      "3 test_acc: 88.75\n",
      "4 test_acc: 89.58\n",
      "5 test_acc: 81.82\n",
      "6 test_acc: 93.94\n",
      "average: 87.35\n",
      "=================RandomForestClassifier=================\n",
      "1 train_acc: 99.66\n",
      "2 train_acc: 99.97\n",
      "3 train_acc: 99.94\n",
      "4 train_acc: 99.85\n",
      "5 train_acc: 99.66\n",
      "6 train_acc: 99.92\n",
      "average: 99.84\n",
      "1 valid_acc: 99.18\n",
      "2 valid_acc: 99.51\n",
      "3 valid_acc: 99.55\n",
      "4 valid_acc: 99.27\n",
      "5 valid_acc: 98.65\n",
      "6 valid_acc: 99.30\n",
      "average: 99.24\n",
      "1 test_acc: 99.39\n",
      "2 test_acc: 97.55\n",
      "3 test_acc: 98.01\n",
      "4 test_acc: 96.98\n",
      "5 test_acc: 93.27\n",
      "6 test_acc: 98.83\n",
      "average: 97.34\n",
      "=================ExtraTreesClassifier=================\n",
      "1 train_acc: 100.00\n",
      "2 train_acc: 100.00\n",
      "3 train_acc: 100.00\n",
      "4 train_acc: 100.00\n",
      "5 train_acc: 100.00\n",
      "6 train_acc: 100.00\n",
      "average: 100.00\n",
      "1 valid_acc: 98.36\n",
      "2 valid_acc: 98.64\n",
      "3 valid_acc: 97.56\n",
      "4 valid_acc: 96.35\n",
      "5 valid_acc: 87.88\n",
      "6 valid_acc: 96.27\n",
      "average: 95.84\n",
      "1 test_acc: 98.36\n",
      "2 test_acc: 97.21\n",
      "3 test_acc: 95.28\n",
      "4 test_acc: 94.97\n",
      "5 test_acc: 91.58\n",
      "6 test_acc: 98.37\n",
      "average: 95.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-02-11 16:31:14,921][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 16:31:15,398][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 16:31:15,900][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 16:31:16,339][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 16:31:16,815][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 16:31:17,266][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-11 16:31:17,267][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 16:31:17,268][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-11 16:31:17,269][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-11 16:31:17,424][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:31:17,599][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:31:17,770][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:31:17,939][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:31:18,123][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:31:18,297][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-11 16:31:18,298][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 16:31:18,299][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-11 16:31:18,300][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-11 16:31:18,458][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:31:18,633][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:31:18,796][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:31:18,971][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:31:19,136][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(6723, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================GCForest=================\n",
      "1 train_acc: 99.66\n",
      "2 train_acc: 99.95\n",
      "3 train_acc: 99.83\n",
      "4 train_acc: 99.97\n",
      "5 train_acc: 100.00\n",
      "6 train_acc: 100.00\n",
      "average: 99.90\n",
      "1 valid_acc: 99.39\n",
      "2 valid_acc: 99.51\n",
      "3 valid_acc: 99.49\n",
      "4 valid_acc: 99.27\n",
      "5 valid_acc: 98.99\n",
      "6 valid_acc: 99.07\n",
      "average: 99.29\n",
      "1 test_acc: 99.39\n",
      "2 test_acc: 97.59\n",
      "3 test_acc: 98.01\n",
      "4 test_acc: 96.98\n",
      "5 test_acc: 93.27\n",
      "6 test_acc: 99.07\n",
      "average: 97.38\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    train_pred = model.predict(X_train2)\n",
    "    valid_pred = model.predict(X_valid)\n",
    "    test_pred = model.predict(X_test)\n",
    "    print(\"=================\" + model_name + \"=================\")\n",
    "    train_cm = confusion_matrix(y_train2, train_pred)\n",
    "    valid_cm = confusion_matrix(y_valid, valid_pred)\n",
    "    test_cm = confusion_matrix(y_test, test_pred)\n",
    "    i=0\n",
    "    train_acc_all = np.zeros(6)\n",
    "    for c in train_cm:\n",
    "        train_acc_all[i] = c[i]/np.sum(c)\n",
    "        print(\"%d train_acc: %.2f\" %(i+1, 100*train_acc_all[i]))\n",
    "        i=i+1\n",
    "    print(\"average: %.2f\" % (100*np.mean(train_acc_all)))\n",
    "    i=0\n",
    "    valid_acc_all = np.zeros(6)\n",
    "    for c in valid_cm:\n",
    "        valid_acc_all[i] = c[i]/np.sum(c)\n",
    "        print(\"%d valid_acc: %.2f\" %(i+1, 100*valid_acc_all[i]))\n",
    "        i=i+1\n",
    "    print(\"average: %.2f\" % (100*np.mean(valid_acc_all)))\n",
    "    i=0\n",
    "    test_acc_all = np.zeros(6)\n",
    "    for c in test_cm:\n",
    "        test_acc_all[i] = c[i]/np.sum(c)\n",
    "        print(\"%d test_acc: %.2f\" %(i+1, 100*test_acc_all[i]))\n",
    "        i=i+1\n",
    "    print(\"average: %.2f\" % (100*np.mean(test_acc_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 计算CE（1-10）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-02-11 16:33:19,455][cascade_classifier.transform] X_groups_test.shape=[(20166, 4)]\n",
      "[ 2019-02-11 16:33:19,458][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 16:33:19,459][cascade_classifier.transform] X_test.shape=(20166, 4)\n",
      "[ 2019-02-11 16:33:19,461][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(20166, 4)\n",
      "[ 2019-02-11 16:33:19,883][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 16:33:20,357][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 16:33:20,794][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 16:33:21,232][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 16:33:21,669][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(20166, 52)\n",
      "[ 2019-02-11 16:33:22,114][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-11 16:33:22,115][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 16:33:22,116][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-11 16:33:22,117][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-11 16:33:22,275][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:33:22,449][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:33:22,616][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:33:22,781][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:33:22,953][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:33:23,123][cascade_classifier.transform] X_groups_test.shape=[(6723, 4)]\n",
      "[ 2019-02-11 16:33:23,124][cascade_classifier.transform] group_dims=[4]\n",
      "[ 2019-02-11 16:33:23,125][cascade_classifier.transform] X_test.shape=(6723, 4)\n",
      "[ 2019-02-11 16:33:23,127][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(6723, 4)\n",
      "[ 2019-02-11 16:33:23,309][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:33:23,483][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:33:23,651][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:33:23,821][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(6723, 52)\n",
      "[ 2019-02-11 16:33:23,990][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(6723, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================CE(1-10)=================\n",
      "1 train_acc: 99.38\n",
      "2 train_acc: 99.86\n",
      "3 train_acc: 99.85\n",
      "4 train_acc: 99.91\n",
      "5 train_acc: 100.00\n",
      "6 train_acc: 100.00\n",
      "average: 99.83\n",
      "1 valid_acc: 99.39\n",
      "2 valid_acc: 99.51\n",
      "3 valid_acc: 99.49\n",
      "4 valid_acc: 99.27\n",
      "5 valid_acc: 98.99\n",
      "6 valid_acc: 99.07\n",
      "average: 99.29\n",
      "1 test_acc: 99.39\n",
      "2 test_acc: 97.63\n",
      "3 test_acc: 98.01\n",
      "4 test_acc: 96.98\n",
      "5 test_acc: 93.27\n",
      "6 test_acc: 99.07\n",
      "average: 97.39\n"
     ]
    }
   ],
   "source": [
    "population_best_weight = np.load(\"../npy/CE_best_weights(1-10).npy\")\n",
    "\n",
    "classifier_num = 10\n",
    "\n",
    "# 所有学习器都输出概率向量，最后投票\n",
    "y_train_pred_proba_all = []\n",
    "y_valid_pred_proba_all = []\n",
    "y_test_pred_proba_all = []\n",
    "\n",
    "# 取训练好的模型，计算各模型”验证集“上输出概率向量\n",
    "for model in models:\n",
    "    train_pred_proba = model.predict_proba(X_train2)\n",
    "    valid_pred_proba = model.predict_proba(X_valid)\n",
    "    test_pred_proba = model.predict_proba(X_test)\n",
    "    y_train_pred_proba_all.append(train_pred_proba)\n",
    "    y_valid_pred_proba_all.append(valid_pred_proba)\n",
    "    y_test_pred_proba_all.append(test_pred_proba)\n",
    "    \n",
    "y_train_pred_ensemble_proba = np.zeros((len(y_train2), 6)) # 初始化集成器概率向量\n",
    "y_valid_pred_ensemble_proba = np.zeros((len(y_valid), 6)) # 初始化集成器概率向量\n",
    "y_test_pred_ensemble_proba = np.zeros((len(y_test), 6)) # 初始化集成器概率向量\n",
    "\n",
    "# 为每一个基学习器乘上权重\n",
    "for k in range(classifier_num):\n",
    "    y_train_pred_ensemble_proba += y_train_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_valid_pred_ensemble_proba += y_valid_pred_proba_all[k] * population_best_weight[k]\n",
    "    y_test_pred_ensemble_proba += y_test_pred_proba_all[k] * population_best_weight[k]\n",
    "y_train_pred_ensemble = np.argmax(y_train_pred_ensemble_proba, axis=1)\n",
    "y_valid_pred_ensemble = np.argmax(y_valid_pred_ensemble_proba, axis=1)\n",
    "y_test_pred_ensemble = np.argmax(y_test_pred_ensemble_proba, axis=1)\n",
    "\n",
    "# 计算各水质等级的得分\n",
    "print(\"=================CE(1-10)=================\")\n",
    "train_cm = confusion_matrix(y_train2, y_train_pred_ensemble)\n",
    "valid_cm = confusion_matrix(y_valid, y_valid_pred_ensemble)\n",
    "test_cm = confusion_matrix(y_test, y_test_pred_ensemble)\n",
    "i=0\n",
    "train_acc_all = np.zeros(6)\n",
    "for c in train_cm:\n",
    "    train_acc_all[i] = c[i]/np.sum(c)\n",
    "    print(\"%d train_acc: %.2f\" %(i+1, 100*train_acc_all[i]))\n",
    "    i=i+1\n",
    "print(\"average: %.2f\" % (100*np.mean(train_acc_all)))\n",
    "i=0\n",
    "valid_acc_all = np.zeros(6)\n",
    "for c in valid_cm:\n",
    "    valid_acc_all[i] = c[i]/np.sum(c)\n",
    "    print(\"%d valid_acc: %.2f\" %(i+1, 100*valid_acc_all[i]))\n",
    "    i=i+1\n",
    "print(\"average: %.2f\" % (100*np.mean(valid_acc_all)))\n",
    "i=0\n",
    "test_acc_all = np.zeros(6)\n",
    "for c in test_cm:\n",
    "    test_acc_all[i] = c[i]/np.sum(c)\n",
    "    print(\"%d test_acc: %.2f\" %(i+1, 100*test_acc_all[i]))\n",
    "    i=i+1\n",
    "print(\"average: %.2f\" % (100*np.mean(test_acc_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gc]",
   "language": "python",
   "name": "conda-env-gc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
